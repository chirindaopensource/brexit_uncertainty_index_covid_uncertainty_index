{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZZVA3r2Q6dJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README.md\n",
        "\n",
        "# Brexit-Related Uncertainty Index (BRUI)\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Statsmodels](https://img.shields.io/badge/Statsmodels-150458.svg?style=flat&logo=python&logoColor=white)](https://www.statsmodels.org/stable/index.html)\n",
        "[![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=flat&logo=Matplotlib&logoColor=black)](https://matplotlib.org/)\n",
        "[![NLTK](https://img.shields.io/badge/NLTK-3776AB.svg?style=flat&logo=python&logoColor=white)](https://www.nltk.org/)\n",
        "[![spaCy](https://img.shields.io/badge/spaCy-09A3D5?style=flat&logo=spacy&logoColor=white)](https://spacy.io/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2507.02439-b31b1b.svg)](https://arxiv.org/abs/2507.02439)\n",
        "[![DOI](https://img.shields.io/badge/DOI-10.48550/arXiv.2507.02439-blue)](https://doi.org/10.48550/arXiv.2507.02439)\n",
        "[![Research](https://img.shields.io/badge/Research-Quantitative%20Finance-green)](https://github.com/chirindaopensource/brexit_uncertainty_index_covid_uncertainty_index)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Econometrics-blue)](https://github.com/chirindaopensource/brexit_uncertainty_index_covid_uncertainty_index)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-NLP%20%26%20VAR-orange)](https://github.com/chirindaopensource/brexit_uncertainty_index_covid_uncertainty_index)\n",
        "[![Text Processing](https://img.shields.io/badge/Text-Processing-blue)](https://spacy.io/)\n",
        "[![Time Series](https://img.shields.io/badge/Time%20Series-Analysis-red)](https://www.statsmodels.org/stable/index.html)\n",
        "[![Data Source](https://img.shields.io/badge/Data%20Source-EIU%20Reports-lightgrey)](https://www.eiu.com/)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/brexit_uncertainty_index_covid_uncertainty_index)\n",
        "\n",
        "**Repository:** https://github.com/chirindaopensource/brexit_uncertainty_index_covid_uncertainty_index\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent** implementation of the research methodology from the 2025 paper entitled **\"Introducing a New Brexit-Related Uncertainty Index: Its Evolution and Economic Consequences\"** by:\n",
        "\n",
        "*   Ismet Gocer\n",
        "*   Julia Darby\n",
        "*   Serdar Ongan\n",
        "\n",
        "The project provides a robust, end-to-end Python pipeline for constructing a high-frequency, quantifiable measure of geopolitical risk, specifically focusing on \"Brexit uncertainty.\" It transforms this abstract concept into a tangible, decision-useful metric and further analyzes its macroeconomic impacts using Vector Autoregression (VAR) models.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: run_brexit_uncertainty_analysis](#key-callable-run_brexit_uncertainty_analysis)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Introducing a New Brexit-Related Uncertainty Index: Its Evolution and Economic Consequences.\" The core of this repository is the iPython Notebook `brexit_related_uncertainty_index_draft.ipynb`, which contains a comprehensive suite of functions to construct the Brexit-Related Uncertainty Index (BRUI) and its complementary COVID-19 Related Uncertainty Index (CRUI), and to analyze their economic consequences.\n",
        "\n",
        "Measuring geopolitical risk is a critical challenge in modern finance and economics. Abstract concepts like \"uncertainty\" must be transformed into quantifiable metrics to be useful for risk pricing, capital allocation, and policy formulation. This framework provides a rigorous, data-driven approach to this problem.\n",
        "\n",
        "This codebase enables researchers, policymakers, and financial analysts to:\n",
        "-   Construct a high-frequency, text-based measure of Brexit uncertainty from raw documents.\n",
        "-   Methodologically disentangle Brexit-related uncertainty from concurrent shocks like the COVID-19 pandemic.\n",
        "-   Analyze the dynamic impact of uncertainty shocks on key macroeconomic variables.\n",
        "-   Replicate and extend the findings of the original research paper.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in a combination of advanced Natural Language Processing (NLP) and standard time-series econometrics:\n",
        "\n",
        "**Context-Aware Uncertainty Attribution:** The core of the index construction is a novel algorithm that moves beyond simple keyword counting. It requires the co-occurrence of an \"uncertainty\" term and a \"Brexit\" term within a small, defined text window (10 words on either side). This ensures that the measured uncertainty is contextually relevant.\n",
        "\n",
        "**Proportional Allocation for Disentanglement:** In periods where both Brexit and COVID-19 are discussed together, the algorithm does not discard the data. Instead, it uses a proportional allocation mechanism based on the relative frequency of \"pure\" Brexit and \"pure\" COVID-19 uncertainty mentions in the same document to disentangle the two effects.\n",
        "\n",
        "**Vector Autoregression (VAR) Modeling:** To assess the economic impact of the newly constructed index, the pipeline employs a standard VAR model. This multivariate time-series model captures the dynamic interdependencies between the BRUI and key macroeconomic variables (GDP, CPI, trade, etc.).\n",
        "\n",
        "**Cholesky Decomposition for Identification:** To identify the causal impact of an uncertainty shock, a Cholesky decomposition is applied to the VAR model's residuals. By ordering the BRUI first in the system, the model operates under the standard economic assumption that uncertainty shocks are contemporaneously exogenousâ€”they affect the economy within the same month, but are not themselves affected by the economy within that same month.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`brexit_related_uncertainty_index_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Parameter Validation:** Rigorous checks for all input data and configurations to ensure methodological compliance.\n",
        "-   **Data Cleansing:** Robust handling of missing and non-finite values, and precise temporal filtering.\n",
        "-   **Advanced NLP Pipeline:** Text normalization, tokenization, context-aware stopword removal, and n-gram analysis.\n",
        "-   **LLM-Powered Entity Recognition:** Use of SpaCy's `en_core_web_lg` to prepare for entity-based analysis.\n",
        "-   **Context-Aware Attribution Algorithm:** The core algorithm for identifying and classifying uncertainty mentions.\n",
        "-   **Index Construction:** Proportional allocation, standardization, and max-normalization to create the final BRUI and CRUI.\n",
        "-   **Econometric Data Preparation:** Systematic stationarity testing (ADF) and data transformations (log, differencing).\n",
        "-   **VAR Modeling:** Automated optimal lag selection, model estimation, and diagnostic testing.\n",
        "-   **Post-Estimation Analysis:** Calculation of Impulse Response Functions (IRFs), Forecast Error Variance Decompositions (FEVDs), and bootstrapped confidence intervals.\n",
        "-   **Publication-Quality Visualization:** A suite of functions to generate the key figures from the paper.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Text Corpus Processing (Steps 1-8):** The pipeline ingests monthly EIU reports, cleans the text, and processes it using a standard NLP workflow (normalization, tokenization, stopword removal).\n",
        "2.  **Context-Aware Attribution (Step 9):** For each \"uncertainty\" keyword, a 21-word context window is analyzed. The presence of Brexit and/or COVID-19 keywords within this window determines the classification of the uncertainty mention.\n",
        "3.  **Proportional Allocation (Step 10):** For \"mixed\" contexts containing both Brexit and COVID-19 keywords, the uncertainty count is allocated proportionally to the BRUI and CRUI based on the relative prevalence of \"pure\" mentions in the same document.\n",
        "4.  **Index Finalization (Step 11):** The raw uncertainty counts are standardized by the total word count of the report and then normalized so that the maximum value of the index over the entire sample period is 100.\n",
        "5.  **Econometric Analysis:** The final BRUI is integrated into a VAR model with key UK macroeconomic variables. The model is used to generate IRFs that trace the economic impact of a one-standard-deviation shock to the BRUI.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `brexit_related_uncertainty_index_draft.ipynb` notebook is structured as a logical pipeline with modular functions for each task:\n",
        "\n",
        "-   **Task 0: `validate_parameters`**: The initial quality gate for all inputs.\n",
        "-   **Task 1: `cleanse_data`**: Handles data quality and temporal scoping.\n",
        "-   **Task 2: `prepare_lexicons`**: Optimizes keyword lists for high-performance matching.\n",
        "-   **Task 3: `preprocess_text_corpus`**: The foundational NLP pipeline.\n",
        "-   **Task 4: `load_spacy_model_for_ner`, `apply_ner_to_corpus`**: LLM-based entity extraction.\n",
        "-   **Task 5: `attribute_uncertainty_in_corpus`**: The core uncertainty attribution algorithm.\n",
        "-   **Task 6 & 7: `calculate_brui`, `calculate_crui`**: Final index construction.\n",
        "-   **Task 8: `prepare_data_for_var`**: Prepares data for econometric modeling.\n",
        "-   **Task 9: `estimate_var_model`**: Estimates and identifies the VAR model.\n",
        "-   **Task 10: `run_post_estimation_analysis`**: Computes IRFs, FEVDs, and confidence intervals.\n",
        "-   **Task 11: `plot_...` functions**: The visualization suite.\n",
        "-   **Main Orchestrator: `run_brexit_uncertainty_analysis`**: Executes the entire pipeline.\n",
        "\n",
        "## Key Callable: run_brexit_uncertainty_analysis\n",
        "\n",
        "The central function in this project is `run_brexit_uncertainty_analysis`. It orchestrates the entire analytical workflow from raw data to final results.\n",
        "\n",
        "```python\n",
        "def run_brexit_uncertainty_analysis(\n",
        "    df_input: pd.DataFrame,\n",
        "    uncertainty_lexicon: List[str],\n",
        "    brexit_lexicon: List[str],\n",
        "    covid_lexicon: List[str],\n",
        "    index_construction_config: Dict[str, Any],\n",
        "    econometric_analysis_config: Dict[str, Any],\n",
        "    brexit_events_for_plotting: Dict[str, str],\n",
        "    comparison_indices_df: pd.DataFrame = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the end-to-end research pipeline for the Brexit Uncertainty Index.\n",
        "\n",
        "    This orchestrator function serves as the master controller for the entire\n",
        "    analysis. It sequentially executes all tasks from parameter validation to\n",
        "    final visualization, ensuring a rigorous, reproducible, and auditable\n",
        "    workflow. Each step's outputs are programmatically passed to the next, and\n",
        "    all significant results, data, and logs are compiled into a comprehensive\n",
        "    final dictionary.\n",
        "\n",
        "    Args:\n",
        "        df_input (pd.DataFrame): The raw input DataFrame containing monthly\n",
        "            macroeconomic data and the EIU text corpus.\n",
        "        uncertainty_lexicon (List[str]): The raw list of uncertainty keywords.\n",
        "        brexit_lexicon (List[str]): The raw list of Brexit-related keywords.\n",
        "        covid_lexicon (List[str]): The raw list of COVID-19 related keywords.\n",
        "        index_construction_config (Dict[str, Any]): Configuration for the\n",
        "            text-based index construction.\n",
        "        econometric_analysis_config (Dict[str, Any]): Configuration for the\n",
        "            econometric VAR analysis.\n",
        "        brexit_events_for_plotting (Dict[str, str]): A dictionary of key Brexit\n",
        "            events for annotating the final BRUI time-series plot.\n",
        "        comparison_indices_df (pd.DataFrame, optional): A DataFrame containing\n",
        "            alternative indices (e.g., BRUI_B, BRUI_C) for validation plotting.\n",
        "            Must have a DatetimeIndex. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing all results.\n",
        "    \"\"\"\n",
        "    # ... (implementation)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `statsmodels`, `matplotlib`, `scipy`, `nltk`, `spacy`.\n",
        "-   NLTK data packages: `punkt`, `stopwords`.\n",
        "-   SpaCy model: `en_core_web_lg`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/brexit_uncertainty_index_covid_uncertainty_index.git\n",
        "    cd brexit_uncertainty_index_covid_uncertainty_index\n",
        "    ```\n",
        "\n",
        "2.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy statsmodels matplotlib scipy nltk spacy\n",
        "    ```\n",
        "\n",
        "3.  **Download required NLP data:**\n",
        "    ```sh\n",
        "    python -m nltk.downloader punkt stopwords\n",
        "    python -m spacy download en_core_web_lg\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The primary input is a `pandas.DataFrame` with the following structure:\n",
        "-   **Index:** A `DatetimeIndex` with monthly frequency ('MS'), covering the full sample period (e.g., '2012-05-01' to '2025-01-01').\n",
        "-   **Columns:**\n",
        "    -   `BRUI`: A placeholder column (e.g., filled with zeros).\n",
        "    -   `GDP`, `CPI`, `PPI`, `X`, `M`, `GBP_EUR`, `GBP_USD`, `EMP`, `UEMP`: Numeric columns containing the macroeconomic data.\n",
        "    -   `EIU`: An object/string column containing the full text of the monthly EIU report.\n",
        "\n",
        "See the usage example for a template of how to construct this DataFrame.\n",
        "\n",
        "## Usage\n",
        "\n",
        "1.  **Prepare Inputs:** Construct the input DataFrame, lexicons, and configuration dictionaries as shown in the detailed usage example within the `brexit_related_uncertainty_index_draft.ipynb` notebook.\n",
        "2.  **Open and Run Notebook:** Open the notebook in a Jupyter environment.\n",
        "3.  **Execute All Cells:** Run all cells in the notebook to define the functions and prepare the example data.\n",
        "4.  **Invoke the Orchestrator:** The final cells of the notebook demonstrate how to call the main `run_brexit_uncertainty_analysis` function with all the prepared inputs.\n",
        "5.  **Analyze Outputs:** The returned dictionary will contain all results, logs, and figures generated by the pipeline.\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `run_brexit_uncertainty_analysis` function returns a single, comprehensive dictionary with the following top-level keys:\n",
        "\n",
        "-   `audit_logs`: Contains detailed logs from the data cleansing, VAR data preparation, and VAR estimation steps.\n",
        "-   `final_data`: Contains key data artifacts, including the `prepared_lexicons` object, the final DataFrame with the `BRUI`, `CRUI`, and all intermediate calculation columns, and the stationary dataset used for the VAR.\n",
        "-   `fitted_model`: Contains the `statsmodels.VARResults` object, which is the complete fitted VAR model.\n",
        "-   `analysis_results`: Contains the results of the post-estimation analysis, including the IRF point estimates, confidence intervals, and FEVD summary tables.\n",
        "-   `visualizations`: Contains the `matplotlib.figure.Figure` objects for the generated plots.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "brexit_uncertainty_index_covid_uncertainty_index/\n",
        "â”‚\n",
        "â”œâ”€â”€ brexit_related_uncertainty_index_draft.ipynb  # Main implementation notebook\n",
        "â”œâ”€â”€ LICENSE                                       # MIT license file\n",
        "â””â”€â”€ README.md                                     # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `index_construction_config` and `econometric_analysis_config` dictionaries. Users can easily modify:\n",
        "-   **Lexicons:** Add or remove keywords from the input lists.\n",
        "-   **Context Window Size:** Change `context_window_size` in the configuration.\n",
        "-   **VAR Model:** Add or remove variables, change the lag selection criterion, or modify the Cholesky ordering in the `econometric_analysis_config`.\n",
        "-   **Post-Estimation:** Adjust the `horizon` for IRFs/FEVDs or the `confidence_interval_level`.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{gocer2025introducing,\n",
        "  title={Introducing a New Brexit-Related Uncertainty Index: Its Evolution and Economic Consequences},\n",
        "  author={Gocer, Ismet and Darby, Julia and Ongan, Serdar},\n",
        "  journal={arXiv preprint arXiv:2507.02439},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Python Implementation of the Brexit-Related Uncertainty Index (BRUI).\n",
        "GitHub repository: https://github.com/chirindaopensource/brexit_uncertainty_index_covid_uncertainty_index\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to Ismet Gocer, Julia Darby, and Serdar Ongan for their novel methodology in constructing a context-aware uncertainty index.\n",
        "-   Thanks to the developers of the `statsmodels`, `pandas`, `spacy`, and `nltk` libraries, which are the foundational pillars of this analytical pipeline.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `brexit_related_uncertainty_index_draft.ipynb` and follows best practices for research software documentation.*\n"
      ],
      "metadata": {
        "id": "eym2-uPIHt_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"Introducing a New Brexit-Related Uncertainty Index: Its Evolution and Economic Consequences\"\n",
        "\n",
        "Link: https://arxiv.org/abs/2507.02439\n",
        "\n",
        "Authors: Notare: Ismet Gocer, Julia Darby, Serdar Ongan\n",
        "\n",
        "Submission Date: 3 Jul 2025\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Important game-changer economic events and transformations cause uncertainties that may affect investment decisions, capital flows, international trade, and macroeconomic variables. One such major transformation is Brexit, which refers to the multiyear process through which the UK withdrew from the EU. This study develops and uses a new Brexit-Related Uncertainty Index (BRUI). In creating this index, we apply Text Mining, Context Window, Natural Language Processing (NLP), and Large Language Models (LLMs) from Deep Learning techniques to analyse the monthly country reports of the Economist Intelligence Unit from May 2012 to January 2025. Additionally, we employ a standard vector autoregression (VAR) analysis to examine the model-implied responses of various macroeconomic variables to BRUI shocks. While developing the BRUI, we also create a complementary COVID-19 Related Uncertainty Index (CRUI) to distinguish the uncertainties stemming from these distinct events. Empirical findings and comparisons of BRUI with other earlier-developed uncertainty indexes demonstrate the robustness of the new index. This new index can assist British policymakers in measuring and understanding the impacts of Brexit-related uncertainties, enabling more effective policy formulation."
      ],
      "metadata": {
        "id": "GtCaIVn3RDWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "#### **Step 1: The Core Problem and the Paper's Stated Contribution**\n",
        "\n",
        "The fundamental problem the authors address is the difficulty in quantifying the *specific* economic uncertainty generated by the multi-year Brexit process. Previous attempts, they argue, have several limitations:\n",
        "*   **Static Measures:** Using a simple dummy variable for \"post-referendum\" is too crude; it doesn't capture the evolving nature and intensity of uncertainty.\n",
        "*   **Conflated Shocks:** Existing uncertainty indices (like the general Economic Policy Uncertainty index) struggle to disentangle Brexit-related anxiety from other major shocks, most notably the COVID-19 pandemic.\n",
        "*   **Limited Scope:** Some specialized Brexit indices are based on surveys (which can be subjective), end too early (e.g., 2016), or haven't been updated.\n",
        "\n",
        "The authors' primary contribution is the creation of a new, dynamic **Brexit-Related Uncertainty Index (BRUI)**. Its key purported advantages are:\n",
        "1.  **Methodological Sophistication:** It uses modern Natural Language Processing (NLP) and a \"Context Window\" approach to ensure uncertainty keywords are directly related to Brexit.\n",
        "2.  **Disentanglement:** It explicitly creates a parallel **COVID-19 Related Uncertainty Index (CRUI)** to methodologically separate and control for pandemic-induced uncertainty.\n",
        "3.  **Temporal Coverage:** The index runs from May 2012 (when the term \"Brexit\" first appeared) to a projected January 2025, covering the entire lifecycle from conception through implementation and its aftermath.\n",
        "4.  **Data Source:** It relies on the Economist Intelligence Unit (EIU) country reports, which are more standardized and analytical than newspaper articles, potentially reducing sensationalism and editorial bias.\n",
        "\n",
        "#### **Step 2: The Methodological Pipeline â€“ From Text to Index**\n",
        "\n",
        "This is the computational core of the paper. The process for creating the BRUI is a multi-stage data processing and analysis pipeline.\n",
        "\n",
        "*   **a. Data Ingestion and Preparation:**\n",
        "    *   The raw data is a corpus of monthly EIU reports for the UK.\n",
        "    *   Using Python libraries (like `PyMuPDF`), they extract the raw text from PDFs and convert it to lowercase for consistency.\n",
        "    *   Standard NLP pre-processing is applied using the Natural Language Toolkit (`NLTK`):\n",
        "        *   **Tokenization:** Breaking text into individual words (tokens).\n",
        "        *   **Stopword Removal:** Eliminating common, non-informative words ('the', 'a', 'is').\n",
        "\n",
        "*   **b. The \"Context Window\" Algorithm (The Key Innovation):**\n",
        "    This is the cleverest part of their methodology. Instead of just counting keyword frequencies in a document, they enforce a *contextual link*.\n",
        "    1.  They define three distinct keyword lexicons (word lists), as shown in their **Table 1**:\n",
        "        *   **Uncertainty Terms (U):** `uncertainty`, `volatile`, `risk`, `instability`, etc.\n",
        "        *   **Brexit-Related Terms (BRK):** `brexit`, `article 50`, `customs union`, `leave the EU`, etc.\n",
        "        *   **COVID-19-Related Terms (CRK):** `covid`, `pandemic`, `lockdown`, `vaccine`, etc.\n",
        "    2.  The algorithm iterates through the text. Whenever it finds an **Uncertainty term (U)**, it opens a \"Context Window\" of 10 words before and 10 words after it.\n",
        "    3.  It then applies a classification logic within this window:\n",
        "        *   If a **BRK** is found in the window but no **CRK** is present, the uncertainty is counted as **purely Brexit-related**.\n",
        "        *   If both a **BRK** and a **CRK** are found, the uncertainty is classified as **mixed**. The authors then allocate this count proportionally based on the overall monthly frequency of pure Brexit vs. pure COVID uncertainty words. This is a pragmatic solution to the attribution problem.\n",
        "\n",
        "*   **c. Index Construction and Normalization:**\n",
        "    1.  For each month, they calculate the **Total Brexit-Related Uncertainty Keyword Number (TBRUKN)**.\n",
        "    2.  To account for varying report lengths, they perform **standardization**:\n",
        "        `BRUI_raw = (TBRUKN for that month) / (Total number of words in that month's report)`\n",
        "    3.  Finally, they perform **normalization**: The entire time series of `BRUI_raw` is scaled so that the peak value is 100. This makes the index easy to interpret and compare over time.\n",
        "\n",
        "#### **Step 3: Empirical Findings â€“ What the Index Reveals**\n",
        "\n",
        "Having constructed the index, the authors use it in two ways: descriptive analysis and econometric modeling.\n",
        "\n",
        "*   **a. The Evolution of Brexit Uncertainty (Figure 2):**\n",
        "    The time-series plot of the BRUI is the paper's central visual. It clearly shows three phases:\n",
        "    1.  **Pre-Brexit Period (2012-2016):** Low-level, rising uncertainty as the referendum is promised and approaches.\n",
        "    2.  **Transition Period (2016-2020):** Extreme volatility and the highest peaks in uncertainty, corresponding to the referendum result, failed withdrawal agreements, the \"Irish backstop\" crisis, and general political chaos.\n",
        "    3.  **Post-Brexit Period (2020-onward):** A lower, but still elevated and persistent, level of uncertainty related to the implementation of new trade rules, the Northern Ireland Protocol, and ongoing economic friction.\n",
        "\n",
        "*   **b. Robustness Checks (Figures 3, 4, 5):**\n",
        "    To validate their index, they correlate it with the other existing Brexit indices.\n",
        "    *   **High Correlation (0.82 and 0.75)** with the Bloom et al. (BRUI_B) and Chung et al. (BRUI_C) indices suggests their BRUI is capturing a similar underlying phenomenon, which lends it credibility.\n",
        "    *   **Low Correlation (0.35)** with the Baker et al. (BRUI_Baker) index is expected and logical, as that index ends in 2016, capturing only the initial, low-level phase of uncertainty.\n",
        "\n",
        "*   **c. Econometric Impact Analysis (The VAR Model):**\n",
        "    This is where they connect the index to the real economy. They use a standard **Vector Autoregression (VAR)** model, which examines the dynamic interrelationships between a set of variables.\n",
        "    *   **Impulse-Response Functions (Figure 6):** This analysis answers the question: \"What happens to the economy when there is a sudden, unexpected shock (a spike) in Brexit uncertainty?\" The results are consistent with economic theory: a positive shock to BRUI leads to a statistically significant *decline* in GDP, exports, imports, and employment, and a *depreciation* of the British Pound against the Euro.\n",
        "    *   **Forecast-Error Variance Decomposition (Table 3):** This quantifies the importance of BRUI shocks. It shows that a meaningful percentage of the forecast errors in key variables like GDP (3.4%), GBP/USD (4.41%), and Imports (2.49%) are explained by fluctuations in the BRUI. This demonstrates that Brexit uncertainty is not just noise but a tangible driver of macroeconomic outcomes.\n",
        "\n",
        "#### **Step 4: Conclusion and Critical Assessment**\n",
        "\n",
        "The paper concludes that the BRUI is a robust and valuable tool. It confirms that Brexit was not a one-off event but a long-term source of structural uncertainty with persistent negative effects on the UK economy.\n",
        "\n",
        "**My professorial critique:**\n",
        "*   **Strengths:** The methodology for disentangling COVID-19 and Brexit uncertainty is the paper's strongest feature. The use of EIU reports is a sound choice for data. The VAR analysis provides a solid econometric foundation for their claims.\n",
        "*   **Limitations (as acknowledged by the authors):** The index is, by definition, sensitive to the initial choice of keywords. While their list is comprehensive, it's not exhaustive. The reliance on a single data source (EIU) could introduce a specific institutional viewpoint, though it aids consistency.\n",
        "*   **Avenues for Future Research:** The proportional allocation for \"mixed\" uncertainty is a reasonable heuristic, but more advanced machine learning models (e.g., topic modeling with attribution) could refine this. Applying this methodology to create sector-specific indices (e.g., for finance, manufacturing) would be a valuable extension.\n",
        "\n",
        "In summary, this is a strong piece of applied econometric and computational work. It provides a tangible, data-driven tool that improves upon existing measures and offers clear, actionable insights into the economic consequences of a major geopolitical event. It's an excellent example of how modern data science techniques can be leveraged to answer pressing economic questions."
      ],
      "metadata": {
        "id": "vpQfEV_qeJTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "ho8lHwjtl2xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Professional Python imports following PEP-8 standards for Brexit uncertainty analysis.\n",
        "\n",
        "This module provides all necessary imports for implementing the Brexit-Related\n",
        "Uncertainty Index (BRUI) methodology, including text processing, statistical\n",
        "analysis, and econometric modeling capabilities.\n",
        "\n",
        "Standard: PEP-8 (https://pep8.org/)\n",
        "Author: CS Chirinda\n",
        "Date: 2025-07-06\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# STANDARD LIBRARY IMPORTS\n",
        "# =============================================================================\n",
        "import re\n",
        "import unicodedata\n",
        "from typing import Any, Dict, List, Set, Tuple, TypedDict\n",
        "\n",
        "# =============================================================================\n",
        "# THIRD-PARTY LIBRARY IMPORTS\n",
        "# =============================================================================\n",
        "# Data manipulation and numerical computing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Natural language processing\n",
        "import nltk\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "# Econometric and time series analysis\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.vector_ar.var_model import VARResults\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n"
      ],
      "metadata": {
        "id": "dkoV5NjHl5uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "QU0qPK1dl82A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Main Constituents of the Draft\n",
        "\n",
        "### **Exegesis of the Research Pipeline Components**\n",
        "\n",
        "#### **1. `validate_parameters` (and its helpers)**\n",
        "*   **Inputs:** Raw user-provided data and configurations: `df_input` (DataFrame), `uncertainty_lexicon` (list), `brexit_lexicon` (list), `covid_lexicon` (list), `index_construction_config` (dict), `econometric_analysis_config` (dict).\n",
        "*   **Process:** This function and its helpers (`_validate_dataframe_structure`, `_validate_lexicons`, etc.) perform a comprehensive, non-destructive validation of all inputs. It traverses the data structures, comparing their properties (e.g., shape, size, data types, values) against a predefined schema derived directly from the research methodology. It aggregates any and all deviations into a list of errors.\n",
        "*   **Output:** `None`. Its successful execution without raising a `ValueError` is a binary signal that all inputs are compliant.\n",
        "*   **Data Transformation:** No data is transformed. This is a pure validation and assertion step.\n",
        "*   **Role in Research Pipeline:** This callable serves as the **Methodological Compliance Gateway**. It ensures that the entire subsequent analysis is built upon a foundation that strictly adheres to the paper's specified sample periods, data formats, lexicons, and algorithmic parameters. It directly enforces the constraints mentioned throughout the methodology, such as the sample period (\"May 2012 and January 2025\"), the keyword lists from **Table 1**, and the algorithmic parameters.\n",
        "\n",
        "#### **2. `cleanse_data`**\n",
        "*   **Inputs:** A validated `pd.DataFrame` and the `index_construction_config` dictionary.\n",
        "*   **Process:** This function executes a three-part data integrity workflow.\n",
        "    1.  **Cleansing:** It scans all non-text columns for a predefined set of string-based null representations (e.g., 'N/A', 'NULL') and replaces them with a standard `np.nan`. It also scans all columns for non-finite numeric values (`np.inf`, `-np.inf`) and replaces them with `np.nan`.\n",
        "    2.  **Filtering:** It filters the DataFrame rows to conform to the precise temporal window specified in the configuration ('2012-05' to '2025-01').\n",
        "    3.  **Verification:** It counts the number of non-null documents in the 'EIU' text column and verifies this count against the expected number from the configuration, raising an error if coverage is insufficient.\n",
        "*   **Output:** A tuple containing: (1) a cleansed, temporally-scoped `pd.DataFrame`, and (2) a detailed audit log (`dict`) of all actions performed.\n",
        "*   **Data Transformation:** The input DataFrame is transformed by (a) value replacement (e.g., 'N/A' -> `np.nan`) and (b) row filtering (slicing by date).\n",
        "*   **Role in Research Pipeline:** This callable implements the **Data Integrity and Scoping** phase. It directly enforces the sample period from **Step 1** of the methodology and prepares the dataset for analysis by standardizing data quality, a crucial but often unstated prerequisite in academic papers.\n",
        "\n",
        "#### **3. `prepare_lexicons`**\n",
        "*   **Inputs:** Three raw `list`s of strings: `uncertainty_lexicon`, `brexit_lexicon`, `covid_lexicon`.\n",
        "*   **Process:** The function transforms the flat lists of keywords into a highly optimized, structured dictionary. For each lexicon, it:\n",
        "    1.  Normalizes all keywords (lowercase, strip whitespace).\n",
        "    2.  Separates them into unigrams (single words) stored in a `set` for O(1) lookup, and n-grams (multi-word phrases).\n",
        "    3.  Organizes the n-grams into a lookup dictionary keyed by their first token. The phrases under each key are sorted by length in descending order to facilitate greedy matching.\n",
        "    4.  It also codifies the \"Scottish referendum\" disambiguation rule from **Table 1, Note 2** into a machine-readable format.\n",
        "*   **Output:** A `PreparedLexicons` dictionary containing the optimized data structures for all three lexicons and the disambiguation rule.\n",
        "*   **Data Transformation:** The input `list`s are transformed into a complex, nested dictionary containing `set`s and other dictionaries, structured for maximum computational performance in the subsequent text analysis.\n",
        "*   **Role in Research Pipeline:** This callable implements the **Lexicon Optimization and Rule Codification**. It is a direct implementation of the n-gram matching requirement (\"'n-grams' used to analyse word sequences, examining bi-grams... and three-grams...\") and the disambiguation rule from **Table 1, Note 2**.\n",
        "\n",
        "#### **4. `preprocess_text_corpus`**\n",
        "*   **Inputs:** The cleansed `pd.DataFrame` from `cleanse_data` and the `PreparedLexicons` object from `prepare_lexicons`.\n",
        "*   **Process:** This function applies a sequential NLP pipeline to the raw text in the 'EIU' column of the DataFrame.\n",
        "    1.  **Normalization:** Converts all text to lowercase and applies Unicode normalization.\n",
        "    2.  **Tokenization:** Splits the normalized text into a list of tokens using `nltk.word_tokenize`.\n",
        "    3.  **Stopword Removal:** Removes common function words, while intelligently preserving any words that appear in the project's keyword lexicons.\n",
        "    4.  **N-gram Generation:** Generates lists of bigrams and trigrams from the cleaned tokens.\n",
        "*   **Output:** The input DataFrame augmented with new columns for each stage of processing: `EIU_lowercase`, `EIU_tokens`, `EIU_cleaned_tokens`, `bigrams`, `trigrams`.\n",
        "*   **Data Transformation:** A string column is transformed into a series of new columns containing progressively processed lists of strings and tuples.\n",
        "*   **Role in Research Pipeline:** This callable implements the **Foundational NLP Pipeline**, directly executing **Steps 3, 4, 5, and 6** of the published methodology.\n",
        "\n",
        "#### **5. `load_spacy_model_for_ner` & `apply_ner_to_corpus`**\n",
        "*   **Inputs:** The preprocessed DataFrame and the model name string (`'en_core_web_lg'`).\n",
        "*   **Process:** `load_spacy_model_for_ner` loads the specified SpaCy model and optimizes it by disabling unused components. `apply_ner_to_corpus` then takes this model and efficiently applies it to the entire text corpus using batch processing (`nlp.pipe`). It extracts the text, label, and character offsets for each named entity found in each document.\n",
        "*   **Output:** The DataFrame augmented with a new `ner_entities` column, where each entry is a list of dictionaries representing the entities found in that document.\n",
        "*   **Data Transformation:** A string column is processed to generate a new column containing structured entity data (lists of dictionaries).\n",
        "*   **Role in Research Pipeline:** These callables implement the **LLM-based Feature Enhancement**, as described in **Step 8** of the methodology: \"This study employs SpaCy's 'en\\_core\\_web\\_lg' module of Large Language Models (LLMs) to elucidate the contextual occurrence of 'uncertainty words (U)'... SpaCy... uses Named Entity Recognition (NER) to identify entities...\". While the paper is vague on the exact use, these functions prepare the necessary data for a concrete enhancement strategy.\n",
        "\n",
        "#### **6. `attribute_uncertainty_in_corpus`**\n",
        "*   **Inputs:** The DataFrame from the previous step, the `PreparedLexicons` object, and the `index_construction_config`.\n",
        "*   **Process:** This is the algorithmic core. It iterates through each document's cleaned tokens. Upon finding an uncertainty keyword, it constructs a context window around it. It then searches this window for Brexit and COVID-19 keywords (using the optimized lexicon structures). Based on the findings, it classifies the uncertainty instance as \"Pure Brexit,\" \"Pure COVID,\" or \"Mixed\" and increments the corresponding counter for that document.\n",
        "*   **Output:** The DataFrame augmented with three new integer columns: `pure_brexit_count`, `pure_covid_count`, and `mixed_count`.\n",
        "*   **Data Transformation:** A column of token lists is transformed into three new columns of numerical counts.\n",
        "*   **Role in Research Pipeline:** This callable implements the **Context-Aware Uncertainty Attribution Algorithm**. It is the direct execution of the logic described in **Step 9** and embodies the following equations:\n",
        "    *   Context Window Construction:\n",
        "        $ \\text{CW} = \\{x_{-10}, \\dots, x_{-1}, U, x_{+1}, \\dots, x_{+10}\\} $\n",
        "    *   Conditional Counting Logic (And the symmetric logic for CRUKN and the case for mixed counts).\n",
        "\n",
        "#### **7. `calculate_brui` & `calculate_crui`**\n",
        "*   **Inputs:** The DataFrame from the attribution step and the `index_construction_config`.\n",
        "*   **Process:** These functions perform the final index calculations.\n",
        "    1.  **Aggregation:** They calculate the total effective uncertainty count for each topic (TBRUKN and TCRUKN) by applying proportional allocation to the `mixed_count`. For BRUI, the formula is `TBRUKN_t = pure_brexit_count_t + (mixed_count_t * brexit_weight_t)`.\n",
        "    2.  **Standardization:** They divide the total count by the document's total word count to get a raw density score.\n",
        "    3.  **Normalization:** They scale the raw density score so that the maximum value of the entire series is 100.\n",
        "*   **Output:** The DataFrame augmented with the final `BRUI` and `CRUI` columns, as well as their intermediate calculation columns (`TBRUKN`, `BRUI_raw`, etc.) for auditability.\n",
        "*   **Data Transformation:** Columns of integer counts are transformed into final, normalized floating-point index values.\n",
        "*   **Role in Research Pipeline:** These callables implement the **Index Finalization**. They directly execute the logic from **Step 10** and **Step 11**, specifically the standardization formula: $ \\text{BRUI}_t = \\frac{\\text{TBRUKN}_t}{(\\text{Total Number of Words Per Report})_t} $\n",
        "    ...followed by the max-to-100 normalization.\n",
        "\n",
        "#### **8. `prepare_data_for_var`**\n",
        "*   **Inputs:** The DataFrame containing the final indices and macroeconomic data, and the `econometric_analysis_config`.\n",
        "*   **Process:** This function prepares the data for econometric modeling. It selects the 10 variables for the VAR, interpolates any missing data, applies log transformations to specified variables, and then applies first-differencing to all variables to induce stationarity. It performs and logs ADF tests both before and after transformation to verify the process.\n",
        "*   **Output:** A tuple containing: (1) the final, stationary `pd.DataFrame` ready for estimation, and (2) a detailed audit log of all tests and transformations.\n",
        "*   **Data Transformation:** The input time series are transformed via logarithmic and differencing operations (`ln(x_t)` and `x_t - x_{t-1}`).\n",
        "*   **Role in Research Pipeline:** This callable implements the **Econometric Data Preparation**. This is a standard but critical step in time-series econometrics, ensuring the data meets the assumptions of the VAR model. It addresses the need for stationarity, a core concept in the field.\n",
        "\n",
        "#### **9. `estimate_var_model`**\n",
        "*   **Inputs:** The stationary DataFrame and the `econometric_analysis_config`.\n",
        "*   **Process:** This function estimates the VAR model. It first selects the optimal lag length `p` using information criteria (AIC, BIC, HQIC). It then fits the VAR(p) model to the data, which must be ordered according to the Cholesky specification. Finally, it performs and logs critical diagnostic tests (for stability, serial correlation, and normality) and computes the Cholesky decomposition of the residual covariance matrix.\n",
        "*   **Output:** A tuple containing: (1) the fitted `statsmodels.VARResults` object, and (2) a detailed log of the estimation process.\n",
        "*   **Data Transformation:** The DataFrame of time-series data is transformed into a fitted statistical model object.\n",
        "*   **Role in Research Pipeline:** This callable implements the **VAR Model Estimation and Identification (Task 9)**. It directly addresses the need for model specification (lag selection) and estimation. Most importantly, it implements the **Cholesky decomposition** identification strategy, which is central to the paper's causal analysis. The ordering of variables, with BRUI first, imposes the economic assumption that uncertainty shocks are contemporaneously exogenous.\n",
        "\n",
        "#### **10. `run_post_estimation_analysis`**\n",
        "*   **Inputs:** The fitted `VARResults` object, the estimation log (containing the Cholesky matrix), and the `econometric_analysis_config`.\n",
        "*   **Process:** This function uses the fitted model to derive economic insights. It computes the Impulse Response Functions (IRFs), the Forecast Error Variance Decompositions (FEVDs), and the percentile bootstrapped confidence intervals for the IRFs.\n",
        "*   **Output:** A dictionary containing the structured results of the IRF, FEVD, and confidence interval calculations.\n",
        "*   **Data Transformation:** The estimated model parameters are transformed into time-path responses (IRFs), variance proportions (FEVDs), and statistical bounds.\n",
        "*   **Role in Research Pipeline:** This callable implements the **Post-Estimation Analysis**. It generates the core analytical outputs of the econometric model, as described in the \"Empirical findings\" section of the paper, which are used to understand the economic consequences of a Brexit uncertainty shock.\n",
        "\n",
        "#### **11. The Visualization Suite (`plot_...` functions)**\n",
        "*   **Inputs:** Various data components from the results dictionary (e.g., the `BRUI` series, the IRF results).\n",
        "*   **Process:** Each function takes specific data and uses the `matplotlib` library to generate a publication-quality plot, styled and annotated according to the figures in the research paper (e.g., Figure 2, Figure 6).\n",
        "*   **Output:** A `matplotlib.figure.Figure` object for each plot.\n",
        "*   **Data Transformation:** Numerical and time-series data are transformed into visual representations (lines, shaded regions, text annotations).\n",
        "*   **Role in Research Pipeline:** These callables implement the **Scientific Visualization**. They are responsible for creating the final, interpretable outputs that communicate the paper's key findings, such as the evolution of the BRUI over time and the dynamic response of the economy to its shocks.\n",
        "\n",
        "#### **12. `run_brexit_uncertainty_analysis`**\n",
        "*   **Inputs:** All raw data and configuration files.\n",
        "*   **Process:** This is the master orchestrator. It calls every other callable in the correct sequence, managing the flow of data from one step to the next. It initializes a master results dictionary and progressively populates it with the outputs and audit logs from each task.\n",
        "*   **Output:** A single, comprehensive dictionary containing all artifacts of the entire research pipeline, from intermediate data to final figures.\n",
        "*   **Data Transformation:** This function orchestrates the entire chain of data transformations.\n",
        "*   **Role in Research Pipeline:** This callable represents the **End-to-End Research Pipeline** itself. It encapsulates the entire methodology of the paper into a single, executable, and reproducible function.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gu121IbQmAYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage Example\n",
        "\n",
        "\n",
        "### **Implementation Example: Executing the End-to-End Brexit Uncertainty Pipeline**\n",
        "\n",
        "This guide demonstrates the practical application of the `run_brexit_uncertainty_analysis` function;\n",
        "i.e. the function which executes the end-to-end research pipeline. A successful execution requires the meticulous preparation of all input parameters. We will construct each parameter as specified in the original project prompt.\n",
        "\n",
        "#### **Step 1: Assembling the Input Data (`df_input`)**\n",
        "\n",
        "The primary input is a `pandas.DataFrame` with a `DatetimeIndex` and precisely defined columns. For this example, we will construct a synthetic DataFrame that mimics the required structure. In a real-world scenario, this DataFrame would be the result of a comprehensive data ingestion process, sourcing macroeconomic data from providers like the ONS and the EIU text reports from their respective archives.\n",
        "\n",
        "```python\n",
        "# Import necessary libraries for data creation.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the precise date range for the analysis as per the methodology.\n",
        "# This covers the period from May 2012 to January 2025 (153 months).\n",
        "date_range = pd.date_range(start=\"2012-05-01\", end=\"2025-01-01\", freq='MS')\n",
        "\n",
        "# Create a dictionary to hold the synthetic data.\n",
        "# The keys must match the required column names exactly.\n",
        "data = {\n",
        "    # The 'BRUI' column is a placeholder; it will be recalculated by the pipeline.\n",
        "    \"BRUI\": np.zeros(len(date_range)),\n",
        "    # Macroeconomic variables are populated with random data for this example.\n",
        "    \"GDP\": np.random.uniform(95, 105, size=len(date_range)),\n",
        "    \"CPI\": np.random.uniform(100, 120, size=len(date_range)),\n",
        "    \"PPI\": np.random.uniform(100, 115, size=len(date_range)),\n",
        "    \"X\": np.random.uniform(25, 35, size=len(date_range)),\n",
        "    \"M\": np.random.uniform(30, 40, size=len(date_range)),\n",
        "    \"GBP_EUR\": np.random.uniform(1.1, 1.3, size=len(date_range)),\n",
        "    \"GBP_USD\": np.random.uniform(1.2, 1.5, size=len(date_range)),\n",
        "    \"EMP\": np.random.uniform(30, 33, size=len(date_range)),\n",
        "    \"UEMP\": np.random.uniform(3.5, 5.5, size=len(date_range)),\n",
        "    # The 'EIU' column holds the text corpus. Here, we use sample sentences.\n",
        "    # In a real application, this would contain the full text of 153 monthly reports.\n",
        "    \"EIU\": [\n",
        "        \"The economic outlook is filled with uncertainty due to the upcoming referendum.\",\n",
        "        \"Discussions surrounding the customs union and single market access are creating tension.\",\n",
        "        \"There is no clarity on the future trade arrangement with the EU.\",\n",
        "        \"The withdrawal agreement faces significant political hurdles, leading to volatility.\",\n",
        "        \"Post-Brexit trade negotiations are precarious. Separately, the pandemic and covid-19 lockdown add to the instability.\"\n",
        "    ] * 30 + [\"Sample text.\"] * 3 # Ensure the list has the correct length.\n",
        "}\n",
        "\n",
        "# Construct the final input DataFrame with the DatetimeIndex.\n",
        "df_input = pd.DataFrame(data, index=date_range)\n",
        "\n",
        "# Display the head and info of the created DataFrame to verify its structure.\n",
        "print(\"--- Input DataFrame Head ---\")\n",
        "print(df_input.head())\n",
        "print(\"\\n--- Input DataFrame Info ---\")\n",
        "df_input.info()\n",
        "```\n",
        "\n",
        "#### **Step 2: Defining the Keyword Lexicons**\n",
        "\n",
        "The next set of inputs are the three keyword lexicons, defined as Python lists of strings. These must match the specifications from Table 1 of the research paper.\n",
        "\n",
        "```python\n",
        "# Define the Uncertainty Lexicon as per the methodology.\n",
        "uncertainty_lexicon = [\n",
        "    \"fear\", \"indecision\", \"instability\", \"jittery\", \"nervousness\",\n",
        "    \"precarious\", \"tense\", \"tension\", \"uncertain\", \"uncertainly\",\n",
        "    \"uncertainty\", \"unclear\", \"unknown\", \"unpredictable\", \"unsettled\",\n",
        "    \"unstable\", \"volatile\", \"volatility\", \"worry\"\n",
        "]\n",
        "\n",
        "# Define the Brexit Lexicon as per the methodology.\n",
        "brexit_lexicon = [\n",
        "    \"article 50\", \"brexit\", \"brexit-related\", \"customs union\", \"eu exit\",\n",
        "    \"eu membership\", \"eu withdrawal\", \"exit deal\", \"exit from the eu\",\n",
        "    \"exit the eu\", \"exit time\", \"exiting\", \"exiting the eu\",\n",
        "    \"exiting the european union\", \"free movement\", \"internal market bill\",\n",
        "    \"leave the eu\", \"northern ireland protocol\", \"post-brexit\", \"pre-brexit\",\n",
        "    \"referendum\", \"regulatory alignment\", \"regulatory framework\", \"single market\",\n",
        "    \"trade arrangement\", \"trade negotiations\", \"transition period\", \"uk exits\",\n",
        "    \"uk-eu relations\", \"uk-eu trade deal\", \"uk's withdrawal\",\n",
        "    \"withdrawal agreement\", \"withdrawal from the eu\"\n",
        "]\n",
        "\n",
        "# Define the COVID-19 Lexicon as per the methodology.\n",
        "covid_lexicon = [\n",
        "    \"coronavirus\", \"covid\", \"covid-19\", \"lockdown\", \"outbreak\",\n",
        "    \"pandemic\", \"quarantine\", \"vaccination\", \"vaccine\"\n",
        "]\n",
        "```\n",
        "\n",
        "#### **Step 3: Defining the Configuration Dictionaries**\n",
        "\n",
        "Two detailed configuration dictionaries are required to control the behavior of the index construction and econometric analysis phases. These parameters are critical for ensuring reproducibility.\n",
        "\n",
        "```python\n",
        "# Define the configuration for the text-based index construction.\n",
        "# This dictionary governs every step from preprocessing to normalization.\n",
        "index_construction_config = {\n",
        "    \"corpus_parameters\": {\n",
        "        \"date_range_start\": \"2012-05\",\n",
        "        \"date_range_end\": \"2025-01\",\n",
        "        \"expected_document_count\": 153\n",
        "    },\n",
        "    \"preprocessing_parameters\": {\n",
        "        \"text_normalization\": \"lowercase\",\n",
        "        \"tokenizer\": \"nltk.word_tokenize\",\n",
        "        \"stopword_language\": \"english\",\n",
        "        \"stopword_source\": \"nltk.corpus.stopwords\"\n",
        "    },\n",
        "    \"algorithm_parameters\": {\n",
        "        \"name\": \"Context-Aware Uncertainty Attribution\",\n",
        "        \"context_window_size\": 10,\n",
        "        \"mixed_count_allocation\": {\n",
        "            \"method\": \"proportional\",\n",
        "            \"fallback\": 0.5\n",
        "        }\n",
        "    },\n",
        "    \"llm_parameters\": {\n",
        "        \"library\": \"spacy\",\n",
        "        \"model_identifier\": \"en_core_web_lg\",\n",
        "        \"purpose\": \"Named Entity Recognition (NER) to enhance keyword identification\"\n",
        "    },\n",
        "    \"finalization_parameters\": {\n",
        "        \"standardization_method\": \"division_by_total_words\",\n",
        "        \"normalization_method\": \"scale_to_max\",\n",
        "        \"normalization_target_max\": 100\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define the configuration for the econometric VAR analysis.\n",
        "# This dictionary controls model specification, data transformation, and post-estimation.\n",
        "econometric_analysis_config = {\n",
        "    \"model_specification\": {\n",
        "        \"model_type\": \"Vector Autoregression (VAR)\",\n",
        "        \"variables\": [\n",
        "            \"BRUI\", \"GDP\", \"CPI\", \"PPI\", \"X\", \"M\",\n",
        "            \"GBP_EUR\", \"GBP_USD\", \"EMP\", \"UEMP\"\n",
        "        ],\n",
        "        \"lag_length_selection_criterion\": [\"AIC\", \"BIC\", \"HQIC\"],\n",
        "        \"include_constant\": True\n",
        "    },\n",
        "    \"data_transformation_parameters\": {\n",
        "        \"log_transform_variables\": [\"GDP\", \"CPI\", \"PPI\", \"X\", \"M\", \"EMP\"],\n",
        "        \"differencing_transform_variables\": \"all\",\n",
        "        \"differencing_order\": 1\n",
        "    },\n",
        "    \"impulse_response_parameters\": {\n",
        "        \"identification_strategy\": \"Cholesky Decomposition\",\n",
        "        \"cholesky_variable_order\": [\n",
        "            \"BRUI\", \"GDP\", \"CPI\", \"PPI\", \"X\", \"M\",\n",
        "            \"GBP_EUR\", \"GBP_USD\", \"EMP\", \"UEMP\"\n",
        "        ],\n",
        "        \"shock_size\": \"one_standard_deviation\",\n",
        "        \"horizon\": 10,\n",
        "        \"confidence_interval_level\": 0.90,\n",
        "        \"confidence_interval_method\": \"standard_percentile_bootstrap\",\n",
        "        \"bootstrap_repetitions\": 999\n",
        "    },\n",
        "    \"fevd_parameters\": {\n",
        "        \"horizon\": 10\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Step 4: Defining Ancillary Data for Visualization**\n",
        "\n",
        "The pipeline requires a dictionary of key events for plotting and can optionally accept a DataFrame of other indices for comparative validation.\n",
        "\n",
        "```python\n",
        "# Define the dictionary of key Brexit events for annotating the BRUI timeline plot.\n",
        "# The keys are date strings, and the values are the descriptions.\n",
        "brexit_events_for_plotting = {\n",
        "    '2016-06-23': 'EU Referendum',\n",
        "    '2017-03-29': 'Article 50 Triggered',\n",
        "    '2019-01-15': \"May's Deal Defeated\",\n",
        "    '2020-01-31': 'UK Leaves EU',\n",
        "    '2020-12-24': 'UK-EU Trade Deal Agreed'\n",
        "}\n",
        "\n",
        "# Define an optional DataFrame for comparative validation plots.\n",
        "# In a real scenario, this data would be loaded from external sources.\n",
        "# For this example, we create synthetic data for BRUI_B and BRUI_C.\n",
        "comparison_data = {\n",
        "    \"BRUI_B\": pd.Series(np.random.uniform(0, 1, size=len(date_range)) * 80, index=date_range),\n",
        "    \"BRUI_C\": pd.Series(np.random.uniform(0, 1, size=len(date_range)) * 90, index=date_range)\n",
        "}\n",
        "# Introduce some NaNs to simulate different temporal coverages.\n",
        "comparison_data[\"BRUI_B\"].iloc[:30] = np.nan\n",
        "comparison_data[\"BRUI_C\"].iloc[-20:] = np.nan\n",
        "comparison_indices_df = pd.DataFrame(comparison_data)\n",
        "```\n",
        "\n",
        "#### **Step 5: Executing the Pipeline**\n",
        "\n",
        "With all inputs meticulously prepared, the final step is to call the master orchestrator function. The function will execute the entire research pipeline and return a comprehensive dictionary containing all results.\n",
        "\n",
        "```python\n",
        "# Before running, ensure all required functions from the previous steps are defined\n",
        "# or imported in the current execution environment. This includes:\n",
        "# validate_parameters, cleanse_data, prepare_lexicons, preprocess_text_corpus,\n",
        "# load_spacy_model_for_ner, apply_ner_to_corpus, attribute_uncertainty_in_corpus,\n",
        "# calculate_brui, calculate_crui, prepare_data_for_var, estimate_var_model,\n",
        "# run_post_estimation_analysis, plot_brui_with_events, plot_comparative_validation,\n",
        "# and plot_impulse_response_functions.\n",
        "\n",
        "# Also ensure NLTK and SpaCy data packages are downloaded:\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# !python -m spacy download en_core_web_lg\n",
        "\n",
        "# Execute the end-to-end analysis pipeline.\n",
        "# The 'try-except' block is a robust way to catch any validation or runtime errors.\n",
        "try:\n",
        "    # Call the main orchestrator function with all prepared inputs.\n",
        "    pipeline_results = run_brexit_uncertainty_analysis(\n",
        "        df_input=df_input,\n",
        "        uncertainty_lexicon=uncertainty_lexicon,\n",
        "        brexit_lexicon=brexit_lexicon,\n",
        "        covid_lexicon=covid_lexicon,\n",
        "        index_construction_config=index_construction_config,\n",
        "        econometric_analysis_config=econometric_analysis_config,\n",
        "        brexit_events_for_plotting=brexit_events_for_plotting,\n",
        "        comparison_indices_df=comparison_indices_df\n",
        "    )\n",
        "\n",
        "    # --- Post-Execution: Accessing Results ---\n",
        "    # The 'pipeline_results' dictionary now contains all artifacts.\n",
        "    \n",
        "    # Example: Access the final DataFrame with the calculated indices.\n",
        "    final_df = pipeline_results['final_data']['indices_and_components']\n",
        "    print(\"\\n--- Final DataFrame with BRUI and CRUI ---\")\n",
        "    print(final_df[['BRUI', 'CRUI']].head())\n",
        "\n",
        "    # Example: Access the VAR estimation log.\n",
        "    var_estimation_log = pipeline_results['audit_logs']['var_estimation']\n",
        "    print(f\"\\n--- VAR Model Optimal Lag (BIC) ---\")\n",
        "    print(var_estimation_log['lag_selection']['optimal_lag_p'])\n",
        "\n",
        "    # Example: Display one of the generated figures.\n",
        "    print(\"\\n--- Displaying Generated BRUI Timeline Plot ---\")\n",
        "    brui_timeline_figure = pipeline_results['visualizations']['brui_timeline']\n",
        "    # In an interactive environment (like Jupyter), this will display the plot.\n",
        "    # To save it, you would use: brui_timeline_figure.savefig('brui_timeline.png')\n",
        "    plt.show()\n",
        "\n",
        "except (ValueError, KeyError, OSError, LookupError) as e:\n",
        "    # Catch potential errors and print them in a structured way.\n",
        "    print(f\"\\n--- AN ERROR OCCURRED ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "\n",
        "```\n",
        "This example provides a complete and reproducible template for using the end-to-end pipeline. It demonstrates how to structure each required input and how to invoke the main function, thereby executing a complex, multi-stage research project with a single, clean function call."
      ],
      "metadata": {
        "id": "fV0AdLpk6PNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 0: Parameter Validation\n",
        "\n",
        "def _validate_dataframe_structure(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the structure, temporal coverage, and dtypes of the input DataFrame.\n",
        "\n",
        "    This function performs a series of checks to ensure the input DataFrame\n",
        "    conforms to the strict requirements of the research methodology. It verifies\n",
        "    the index type, date range, row count, column names, and data types for\n",
        "    each column.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing macroeconomic and EIU data.\n",
        "        config (Dict[str, Any]): The index construction configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages. An empty list indicates success.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate validation error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # --- Step 1.1: Validate DataFrame Index ---\n",
        "    # The index must be a pandas DatetimeIndex for time-series operations.\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
        "        errors.append(\"DataFrame index is not a DatetimeIndex.\")\n",
        "        # If the index is not a datetime object, further date checks are invalid.\n",
        "        return errors\n",
        "\n",
        "    # --- Step 1.2: Validate Temporal Coverage ---\n",
        "    try:\n",
        "        # Extract expected start and end dates from the configuration.\n",
        "        expected_start_str = config['corpus_parameters']['date_range_start']\n",
        "        expected_end_str = config['corpus_parameters']['date_range_end']\n",
        "\n",
        "        # Convert configuration strings to pandas Timestamp objects for comparison.\n",
        "        expected_start = pd.to_datetime(expected_start_str)\n",
        "        expected_end = pd.to_datetime(expected_end_str)\n",
        "\n",
        "        # Compare the DataFrame's actual date range with the expected range.\n",
        "        if df.index.min() != expected_start:\n",
        "            errors.append(\n",
        "                f\"DataFrame start date {df.index.min().strftime('%Y-%m')} \"\n",
        "                f\"does not match expected start date {expected_start_str}.\"\n",
        "            )\n",
        "        if df.index.max() != expected_end:\n",
        "            errors.append(\n",
        "                f\"DataFrame end date {df.index.max().strftime('%Y-%m')} \"\n",
        "                f\"does not match expected end date {expected_end_str}.\"\n",
        "            )\n",
        "    except KeyError as e:\n",
        "        errors.append(f\"Missing required date key in config: {e}\")\n",
        "    except Exception as e:\n",
        "        errors.append(f\"Error processing date range from config: {e}\")\n",
        "\n",
        "    # --- Step 1.3: Validate Document/Row Count ---\n",
        "    try:\n",
        "        # The number of rows must match the expected number of monthly reports.\n",
        "        expected_count = config['corpus_parameters']['expected_document_count']\n",
        "        if len(df) != expected_count:\n",
        "            errors.append(\n",
        "                f\"DataFrame has {len(df)} rows, but expected document \"\n",
        "                f\"count is {expected_count}.\"\n",
        "            )\n",
        "    except KeyError as e:\n",
        "        errors.append(f\"Missing 'expected_document_count' in config: {e}\")\n",
        "\n",
        "    # --- Step 1.4: Validate Column Structure ---\n",
        "    # Define the exact set of required columns as per the methodology.\n",
        "    expected_columns: Set[str] = {\n",
        "        \"BRUI\", \"GDP\", \"CPI\", \"PPI\", \"X\", \"M\", \"GBP_EUR\",\n",
        "        \"GBP_USD\", \"EMP\", \"UEMP\", \"EIU\"\n",
        "    }\n",
        "    # Compare the set of actual columns to the expected set.\n",
        "    if set(df.columns) != expected_columns:\n",
        "        errors.append(\n",
        "            f\"DataFrame columns mismatch. Expected: {sorted(list(expected_columns))}, \"\n",
        "            f\"Got: {sorted(list(df.columns))}.\"\n",
        "        )\n",
        "\n",
        "    # --- Step 1.5: Validate Column Data Types ---\n",
        "    # Define the expected data type for each column.\n",
        "    expected_dtypes: Dict[str, str] = {\n",
        "        \"BRUI\": \"numeric\", \"GDP\": \"numeric\", \"CPI\": \"numeric\", \"PPI\": \"numeric\",\n",
        "        \"X\": \"numeric\", \"M\": \"numeric\", \"GBP_EUR\": \"numeric\", \"GBP_USD\": \"numeric\",\n",
        "        \"EMP\": \"numeric\", \"UEMP\": \"numeric\", \"EIU\": \"object\"\n",
        "    }\n",
        "    # Iterate through expected columns to check their types.\n",
        "    for col, expected_type in expected_dtypes.items():\n",
        "        # Check if the column exists before trying to access its dtype.\n",
        "        if col in df.columns:\n",
        "            # Check for numeric types.\n",
        "            if expected_type == \"numeric\":\n",
        "                if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                    errors.append(\n",
        "                        f\"Column '{col}' is not numeric. \"\n",
        "                        f\"Found dtype: {df[col].dtype}.\"\n",
        "                    )\n",
        "            # Check for object type (for strings).\n",
        "            elif expected_type == \"object\":\n",
        "                if not pd.api.types.is_object_dtype(df[col]):\n",
        "                    errors.append(\n",
        "                        f\"Column '{col}' is not of object type (for strings). \"\n",
        "                        f\"Found dtype: {df[col].dtype}.\"\n",
        "                    )\n",
        "\n",
        "    # Return the list of all found errors.\n",
        "    return errors\n",
        "\n",
        "def _validate_lexicons(\n",
        "    uncertainty_lexicon: List[str],\n",
        "    brexit_lexicon: List[str],\n",
        "    covid_lexicon: List[str],\n",
        "    expected_counts: Dict[str, int] = {'Uncertainty': 19,\n",
        "                                       'Brexit': 33,\n",
        "                                       'COVID-19': 9}) -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the keyword lexicons against specified counts and structural rules.\n",
        "\n",
        "    This function performs a series of rigorous checks to ensure the keyword\n",
        "    lexicons are correctly structured. It validates:\n",
        "    1.  The size of each lexicon against explicitly provided expected counts.\n",
        "    2.  The data type of each element, ensuring all are strings.\n",
        "    3.  The internal uniqueness of each lexicon (i.e., no duplicate keywords).\n",
        "    4.  The external uniqueness between lexicons (i.e., no overlapping keywords).\n",
        "    This function is designed to be a pure validator, decoupled from hard-coded\n",
        "    project data.\n",
        "\n",
        "    Args:\n",
        "        uncertainty_lexicon (List[str]): The list of uncertainty-related keywords.\n",
        "        brexit_lexicon (List[str]): The list of Brexit-related keywords.\n",
        "        covid_lexicon (List[str]): The list of COVID-19 related keywords.\n",
        "        expected_counts (Dict[str, int]): A dictionary specifying the exact\n",
        "            expected number of keywords for each lexicon (e.g.,\n",
        "            {'Uncertainty': 19, 'Brexit': 33, 'COVID-19': 9}).\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of string error messages. An empty list indicates\n",
        "            that all validation checks passed successfully.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate all validation error messages found.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Create a dictionary to map lexicon names to their list objects for iteration.\n",
        "    lexicons = {\n",
        "        \"Uncertainty\": uncertainty_lexicon,\n",
        "        \"Brexit\": brexit_lexicon,\n",
        "        \"COVID-19\": covid_lexicon\n",
        "    }\n",
        "\n",
        "    # Initialize a dictionary to hold the processed sets of keywords for overlap checks.\n",
        "    processed_sets: Dict[str, Set[str]] = {}\n",
        "\n",
        "    # --- Iterate through each lexicon to perform validation checks ---\n",
        "    for name, lexicon in lexicons.items():\n",
        "        # Verify that the input object is a list.\n",
        "        if not isinstance(lexicon, list):\n",
        "            # If not a list, append a fatal error for this lexicon and skip further checks.\n",
        "            errors.append(f\"Lexicon '{name}' is not a list, but type {type(lexicon).__name__}.\")\n",
        "            # Continue to the next lexicon in the loop.\n",
        "            continue\n",
        "\n",
        "        # Check if the size of the lexicon matches the expected count passed as a parameter.\n",
        "        if len(lexicon) != expected_counts.get(name, -1):\n",
        "            # Append an error message if the counts do not match.\n",
        "            errors.append(\n",
        "                f\"Lexicon '{name}' has {len(lexicon)} items, but \"\n",
        "                f\"expected {expected_counts.get(name)}.\"\n",
        "            )\n",
        "\n",
        "        # --- Process and validate each keyword within the lexicon ---\n",
        "        # Use a temporary list to store processed keywords for this lexicon.\n",
        "        processed_keywords: List[str] = []\n",
        "        # Use enumerate to track the index for precise error reporting.\n",
        "        for i, kw in enumerate(lexicon):\n",
        "            # Remediation: Implement robust type checking for each element.\n",
        "            # Verify that the keyword is a string before attempting string operations.\n",
        "            if not isinstance(kw, str):\n",
        "                # If not a string, log a specific error with the index and value.\n",
        "                errors.append(\n",
        "                    f\"Lexicon '{name}' contains a non-string element at index {i}: '{kw}' \"\n",
        "                    f\"(type: {type(kw).__name__}).\"\n",
        "                )\n",
        "                # Skip processing for this invalid element.\n",
        "                continue\n",
        "\n",
        "            # Normalize the keyword by converting to lowercase and stripping whitespace.\n",
        "            processed_kw = kw.lower().strip()\n",
        "            # Add the cleaned keyword to our list for this lexicon.\n",
        "            processed_keywords.append(processed_kw)\n",
        "\n",
        "        # Check for internal duplicates by comparing the length of the list of\n",
        "        # processed keywords to the length of a set made from the same list.\n",
        "        if len(set(processed_keywords)) != len(processed_keywords):\n",
        "            # Append an error if duplicates are found within the lexicon.\n",
        "            errors.append(f\"Lexicon '{name}' contains duplicate entries after normalization.\")\n",
        "\n",
        "        # Store the final, clean set of keywords for inter-lexicon overlap checks.\n",
        "        processed_sets[name] = set(processed_keywords)\n",
        "\n",
        "    # --- Perform inter-lexicon overlap checks if all lexicons were valid lists ---\n",
        "    # This check is only meaningful if all three lexicons were successfully processed into sets.\n",
        "    if len(processed_sets) == 3:\n",
        "        # Check for any common keywords between the Uncertainty and Brexit sets.\n",
        "        if not processed_sets[\"Uncertainty\"].isdisjoint(processed_sets[\"Brexit\"]):\n",
        "            # If intersection is not empty, log an error.\n",
        "            overlap = processed_sets[\"Uncertainty\"] & processed_sets[\"Brexit\"]\n",
        "            errors.append(f\"Overlap found between Uncertainty and Brexit lexicons: {overlap}\")\n",
        "\n",
        "        # Check for any common keywords between the Uncertainty and COVID-19 sets.\n",
        "        if not processed_sets[\"Uncertainty\"].isdisjoint(processed_sets[\"COVID-19\"]):\n",
        "            # If intersection is not empty, log an error.\n",
        "            overlap = processed_sets[\"Uncertainty\"] & processed_sets[\"COVID-19\"]\n",
        "            errors.append(f\"Overlap found between Uncertainty and COVID-19 lexicons: {overlap}\")\n",
        "\n",
        "        # Check for any common keywords between the Brexit and COVID-19 sets.\n",
        "        if not processed_sets[\"Brexit\"].isdisjoint(processed_sets[\"COVID-19\"]):\n",
        "            # If intersection is not empty, log an error.\n",
        "            overlap = processed_sets[\"Brexit\"] & processed_sets[\"COVID-19\"]\n",
        "            errors.append(f\"Overlap found between Brexit and COVID-19 lexicons: {overlap}\")\n",
        "\n",
        "    # Return the final, aggregated list of all validation errors.\n",
        "    return errors\n",
        "\n",
        "def _validate_index_construction_config(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the index construction config against methodological requirements.\n",
        "\n",
        "    This function performs a deep check of the nested configuration dictionary\n",
        "    to ensure all parameters for text processing, algorithm choice, and\n",
        "    index finalization align with the research paper's steps.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The index construction configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages. An empty list indicates success.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate validation error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Define a schema of expected values for validation.\n",
        "    expected_schema = {\n",
        "        \"corpus_parameters\": {\n",
        "            \"date_range_start\": \"2012-05\",\n",
        "            \"date_range_end\": \"2025-01\",\n",
        "            \"expected_document_count\": 153\n",
        "        },\n",
        "        \"preprocessing_parameters\": {\n",
        "            \"text_normalization\": \"lowercase\",\n",
        "            \"tokenizer\": \"nltk.word_tokenize\",\n",
        "            \"stopword_language\": \"english\",\n",
        "            \"stopword_source\": \"nltk.corpus.stopwords\"\n",
        "        },\n",
        "        \"algorithm_parameters\": {\n",
        "            \"name\": \"Context-Aware Uncertainty Attribution\",\n",
        "            \"context_window_size\": 10,\n",
        "            \"mixed_count_allocation\": {\n",
        "                \"method\": \"proportional\",\n",
        "                \"fallback\": 0.5\n",
        "            }\n",
        "        },\n",
        "        \"llm_parameters\": {\n",
        "            \"library\": \"spacy\",\n",
        "            \"model_identifier\": \"en_core_web_lg\",\n",
        "            \"purpose\": \"Named Entity Recognition (NER) to enhance keyword identification\"\n",
        "        },\n",
        "        \"finalization_parameters\": {\n",
        "            \"standardization_method\": \"division_by_total_words\",\n",
        "            \"normalization_method\": \"scale_to_max\",\n",
        "            \"normalization_target_max\": 100\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # --- Step 3.1: Validate the configuration structure and values ---\n",
        "    # Iterate through the expected schema to check each parameter.\n",
        "    for key, sub_schema in expected_schema.items():\n",
        "        # Check if the top-level key exists in the provided config.\n",
        "        if key not in config:\n",
        "            errors.append(f\"Missing top-level key '{key}' in index_construction_config.\")\n",
        "            continue\n",
        "\n",
        "        # Get the sub-dictionary from the user's config.\n",
        "        sub_config = config[key]\n",
        "        # Iterate through the nested schema.\n",
        "        for sub_key, expected_value in sub_schema.items():\n",
        "            # Check for nested dictionaries.\n",
        "            if isinstance(expected_value, dict):\n",
        "                if sub_key not in sub_config:\n",
        "                    errors.append(f\"Missing nested key '{sub_key}' in '{key}'.\")\n",
        "                    continue\n",
        "                # Iterate through the second level of nesting.\n",
        "                for nested_key, nested_expected_value in expected_value.items():\n",
        "                    if nested_key not in sub_config[sub_key]:\n",
        "                        errors.append(f\"Missing key '{nested_key}' in '{key}.{sub_key}'.\")\n",
        "                        continue\n",
        "                    # Compare the actual value with the expected value.\n",
        "                    actual_value = sub_config[sub_key][nested_key]\n",
        "                    if actual_value != nested_expected_value:\n",
        "                        errors.append(\n",
        "                            f\"Config mismatch at '{key}.{sub_key}.{nested_key}'. \"\n",
        "                            f\"Expected: {nested_expected_value}, Got: {actual_value}.\"\n",
        "                        )\n",
        "            else:\n",
        "                # Check for keys in the first level of nesting.\n",
        "                if sub_key not in sub_config:\n",
        "                    errors.append(f\"Missing key '{sub_key}' in '{key}'.\")\n",
        "                    continue\n",
        "                # Compare the actual value with the expected value.\n",
        "                actual_value = sub_config[sub_key]\n",
        "                if actual_value != expected_value:\n",
        "                    errors.append(\n",
        "                        f\"Config mismatch at '{key}.{sub_key}'. \"\n",
        "                        f\"Expected: {expected_value}, Got: {actual_value}.\"\n",
        "                    )\n",
        "\n",
        "    # Return the list of all found errors.\n",
        "    return errors\n",
        "\n",
        "def _validate_econometric_analysis_config(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the econometric analysis config against VAR methodology.\n",
        "\n",
        "    This function ensures the VAR model specification, data transformations,\n",
        "    and identification strategy (especially the Cholesky ordering) are\n",
        "    consistent with the research paper's econometric design.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The econometric analysis configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages. An empty list indicates success.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate validation error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # --- Step 4.1: Validate Model Specification ---\n",
        "    try:\n",
        "        spec = config['model_specification']\n",
        "        # Check model type.\n",
        "        if spec['model_type'] != \"Vector Autoregression (VAR)\":\n",
        "            errors.append(\"model_type must be 'Vector Autoregression (VAR)'.\")\n",
        "        # Check variable list.\n",
        "        expected_vars = {\n",
        "            \"BRUI\", \"GDP\", \"CPI\", \"PPI\", \"X\", \"M\",\n",
        "            \"GBP_EUR\", \"GBP_USD\", \"EMP\", \"UEMP\"\n",
        "        }\n",
        "        if set(spec['variables']) != expected_vars:\n",
        "            errors.append(\"Variable list in model_specification is incorrect.\")\n",
        "        # Check lag selection criteria.\n",
        "        if set(spec['lag_length_selection_criterion']) != {\"AIC\", \"BIC\", \"HQIC\"}:\n",
        "            errors.append(\"lag_length_selection_criterion is incorrect.\")\n",
        "        # Check for constant term.\n",
        "        if spec['include_constant'] is not True:\n",
        "            errors.append(\"include_constant must be True.\")\n",
        "    except KeyError as e:\n",
        "        errors.append(f\"Missing key in model_specification: {e}\")\n",
        "\n",
        "    # --- Step 4.2: Validate Data Transformation Parameters ---\n",
        "    try:\n",
        "        trans = config['data_transformation_parameters']\n",
        "        # Check log transform variables.\n",
        "        expected_log_vars = {\"GDP\", \"CPI\", \"PPI\", \"X\", \"M\", \"EMP\"}\n",
        "        if set(trans['log_transform_variables']) != expected_log_vars:\n",
        "            errors.append(\"log_transform_variables list is incorrect.\")\n",
        "        # Check differencing specification.\n",
        "        if trans['differencing_transform_variables'] != \"all\":\n",
        "            errors.append(\"differencing_transform_variables must be 'all'.\")\n",
        "        # Check differencing order.\n",
        "        if trans['differencing_order'] != 1:\n",
        "            errors.append(\"differencing_order must be 1.\")\n",
        "    except KeyError as e:\n",
        "        errors.append(f\"Missing key in data_transformation_parameters: {e}\")\n",
        "\n",
        "    # --- Step 4.3: Validate Impulse Response Parameters ---\n",
        "    try:\n",
        "        ir_params = config['impulse_response_parameters']\n",
        "        # Check identification strategy.\n",
        "        if ir_params['identification_strategy'] != \"Cholesky Decomposition\":\n",
        "            errors.append(\"identification_strategy must be 'Cholesky Decomposition'.\")\n",
        "\n",
        "        # CRITICAL: Validate Cholesky variable ordering.\n",
        "        cholesky_order = ir_params['cholesky_variable_order']\n",
        "        # Check that BRUI is the first variable. This is a core assumption.\n",
        "        if not cholesky_order or cholesky_order[0] != \"BRUI\":\n",
        "            errors.append(\"Cholesky order is invalid: 'BRUI' must be the first variable.\")\n",
        "        # Check that the set of variables in the order matches the model spec.\n",
        "        if set(cholesky_order) != set(config['model_specification']['variables']):\n",
        "            errors.append(\"Cholesky order variables do not match model_specification variables.\")\n",
        "\n",
        "        # Check other IRF parameters.\n",
        "        if ir_params['shock_size'] != \"one_standard_deviation\":\n",
        "            errors.append(\"shock_size must be 'one_standard_deviation'.\")\n",
        "        if ir_params['horizon'] != 10:\n",
        "            errors.append(\"horizon must be 10.\")\n",
        "        if ir_params['confidence_interval_level'] != 0.90:\n",
        "            errors.append(\"confidence_interval_level must be 0.90.\")\n",
        "        if ir_params['confidence_interval_method'] != \"standard_percentile_bootstrap\":\n",
        "            errors.append(\"confidence_interval_method must be 'standard_percentile_bootstrap'.\")\n",
        "        if ir_params['bootstrap_repetitions'] != 999:\n",
        "            errors.append(\"bootstrap_repetitions must be 999.\")\n",
        "    except KeyError as e:\n",
        "        errors.append(f\"Missing key in impulse_response_parameters: {e}\")\n",
        "\n",
        "    # --- Step 4.4: Validate FEVD Parameters ---\n",
        "    try:\n",
        "        fevd_params = config['fevd_parameters']\n",
        "        # Check FEVD horizon.\n",
        "        if fevd_params['horizon'] != 10:\n",
        "            errors.append(\"FEVD horizon must be 10.\")\n",
        "    except KeyError as e:\n",
        "        errors.append(f\"Missing key in fevd_parameters: {e}\")\n",
        "\n",
        "    # Return the list of all found errors.\n",
        "    return errors\n",
        "\n",
        "def validate_parameters(\n",
        "    df: pd.DataFrame,\n",
        "    uncertainty_lexicon: List[str],\n",
        "    brexit_lexicon: List[str],\n",
        "    covid_lexicon: List[str],\n",
        "    index_construction_config: Dict[str, Any],\n",
        "    econometric_analysis_config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of all input parameters for the BRUI project.\n",
        "\n",
        "    This function serves as the main entry point for parameter validation. It\n",
        "    calls specialized helper functions to validate each component: the input\n",
        "    DataFrame, the keyword lexicons, the index construction configuration, and\n",
        "    the econometric analysis configuration. If any validation check fails, it\n",
        "    aggregates all error messages and raises a single, comprehensive\n",
        "    ValueError, allowing the user to correct all issues at once.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The primary DataFrame with monthly data.\n",
        "        uncertainty_lexicon (List[str]): List of uncertainty-related keywords.\n",
        "        brexit_lexicon (List[str]): List of Brexit-related keywords.\n",
        "        covid_lexicon (List[str]): List of COVID-19 related keywords.\n",
        "        index_construction_config (Dict[str, Any]): Configuration for BRUI calculation.\n",
        "        econometric_analysis_config (Dict[str, Any]): Configuration for VAR analysis.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If any input parameter is of an incorrect type.\n",
        "        ValueError: If any validation check fails, containing a detailed\n",
        "                    list of all identified issues.\n",
        "\n",
        "    Returns:\n",
        "        None: The function returns None if all validations pass.\n",
        "    \"\"\"\n",
        "    # --- Type Checking for main inputs ---\n",
        "    # Ensure all inputs are of the expected high-level type.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(uncertainty_lexicon, list):\n",
        "        raise TypeError(\"Input 'uncertainty_lexicon' must be a list.\")\n",
        "    if not isinstance(brexit_lexicon, list):\n",
        "        raise TypeError(\"Input 'brexit_lexicon' must be a list.\")\n",
        "    if not isinstance(covid_lexicon, list):\n",
        "        raise TypeError(\"Input 'covid_lexicon' must be a list.\")\n",
        "    if not isinstance(index_construction_config, dict):\n",
        "        raise TypeError(\"Input 'index_construction_config' must be a dictionary.\")\n",
        "    if not isinstance(econometric_analysis_config, dict):\n",
        "        raise TypeError(\"Input 'econometric_analysis_config' must be a dictionary.\")\n",
        "\n",
        "    # --- Aggregate errors from all validation steps ---\n",
        "    # Initialize a list to hold all error messages from all checks.\n",
        "    all_errors: List[str] = []\n",
        "\n",
        "    # Execute DataFrame validation.\n",
        "    all_errors.extend(_validate_dataframe_structure(df, index_construction_config))\n",
        "\n",
        "    # Execute lexicon validation.\n",
        "    all_errors.extend(_validate_lexicons(\n",
        "        uncertainty_lexicon, brexit_lexicon, covid_lexicon\n",
        "    ))\n",
        "\n",
        "    # Execute index construction config validation.\n",
        "    all_errors.extend(_validate_index_construction_config(index_construction_config))\n",
        "\n",
        "    # Execute econometric analysis config validation.\n",
        "    all_errors.extend(_validate_econometric_analysis_config(econometric_analysis_config))\n",
        "\n",
        "    # --- Final Error Reporting ---\n",
        "    # If the list of errors is not empty, raise a single, comprehensive ValueError.\n",
        "    if all_errors:\n",
        "        # Format the error messages for clear presentation.\n",
        "        error_header = \"Parameter validation failed with the following errors:\"\n",
        "        formatted_errors = \"\\n\".join([f\"  - {error}\" for error in all_errors])\n",
        "        # Raise the exception.\n",
        "        raise ValueError(f\"{error_header}\\n{formatted_errors}\")\n",
        "\n",
        "    # If no errors were found, the function completes successfully.\n",
        "    # A print statement can be used for explicit confirmation in an interactive session.\n",
        "    # print(\"All parameters successfully validated against the research methodology.\")\n"
      ],
      "metadata": {
        "id": "LhLnRHj9mCWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Data Cleansing\n",
        "\n",
        "def cleanse_data(\n",
        "    df: pd.DataFrame,\n",
        "    index_construction_config: Dict[str, Any],\n",
        "    missing_doc_threshold: float = 0.95,\n",
        "    text_column: str = 'EIU'\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs systematic data cleansing, temporal filtering, and corpus validation.\n",
        "\n",
        "    This function executes a rigorous, multi-step data cleansing pipeline. It\n",
        "    is designed to be non-destructive to the original text corpus while\n",
        "    thoroughly cleaning all other columns. The pipeline includes:\n",
        "    1.  A targeted replacement strategy to standardize missing and non-finite\n",
        "        values, applying string-based null replacements only to non-text columns.\n",
        "    2.  Precise temporal filtering to align the DataFrame with the research\n",
        "        methodology's sample period (May 2012 - Jan 2025).\n",
        "    3.  Strict verification of the EIU text corpus completeness, raising an\n",
        "        error if coverage falls below a critical threshold.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame, assumed to have passed the\n",
        "            validations in Task 0.\n",
        "        index_construction_config (Dict[str, Any]): The configuration dictionary\n",
        "            containing corpus parameters like date ranges and expected counts.\n",
        "        missing_doc_threshold (float): The minimum required proportion of\n",
        "            documents. If coverage falls below this, a ValueError is raised.\n",
        "            Defaults to 0.95.\n",
        "        text_column (str): The name of the column containing the raw text corpus,\n",
        "            which will be excluded from string-based null replacements.\n",
        "            Defaults to 'EIU'.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]: A tuple containing:\n",
        "            - The cleansed, temporally filtered pandas DataFrame.\n",
        "            - A detailed audit log dictionary documenting all actions taken.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the EIU document coverage is below the specified\n",
        "            `missing_doc_threshold`.\n",
        "        KeyError: If required keys are missing from the configuration dictionary.\n",
        "    \"\"\"\n",
        "    # Create a deep copy to avoid modifying the original DataFrame.\n",
        "    df_cleaned = df.copy()\n",
        "\n",
        "    # Initialize a dictionary to serve as a detailed audit log for all operations.\n",
        "    audit_log: Dict[str, Any] = {\n",
        "        \"data_quality_assessment\": {},\n",
        "        \"temporal_filtering\": {},\n",
        "        \"document_count_verification\": {}\n",
        "    }\n",
        "\n",
        "    # --- Step 1: Comprehensive Data Quality Assessment and Targeted Replacement ---\n",
        "    # Remediation: Implement a column-targeted replacement strategy.\n",
        "\n",
        "    # Define lists of values to replace, separated by type.\n",
        "    numeric_nulls = [np.inf, -np.inf]\n",
        "    string_nulls = ['', 'N/A', 'NULL', 'null', 'None']\n",
        "\n",
        "    # Identify the columns to which string replacements will be applied.\n",
        "    # This excludes the specified text_column to preserve its integrity.\n",
        "    non_text_columns = [col for col in df_cleaned.columns if col != text_column]\n",
        "\n",
        "    # --- Audit and Replace ---\n",
        "    # Initialize a dictionary to store the detailed audit of replacements.\n",
        "    replacement_audit = {}\n",
        "\n",
        "    # Stage 1: Audit and replace numeric nulls across the entire DataFrame.\n",
        "    for value in numeric_nulls:\n",
        "        # Find where the value occurs.\n",
        "        locations = df_cleaned == value\n",
        "        # Count occurrences per column.\n",
        "        counts_per_column = locations.sum()\n",
        "        # Filter to include only columns with one or more occurrences.\n",
        "        affected_columns = counts_per_column[counts_per_column > 0].to_dict()\n",
        "        # If any columns are affected, log the details.\n",
        "        if affected_columns:\n",
        "            replacement_audit[str(value)] = {\n",
        "                \"total_replacements\": int(locations.sum().sum()),\n",
        "                \"columns_affected\": affected_columns\n",
        "            }\n",
        "    # Perform the replacement for numeric nulls.\n",
        "    df_cleaned.replace(numeric_nulls, np.nan, inplace=True)\n",
        "\n",
        "    # Stage 2: Audit and replace string-based nulls ONLY in non-text columns.\n",
        "    for value in string_nulls:\n",
        "        # Find where the value occurs, but only in the target columns.\n",
        "        locations = df_cleaned[non_text_columns] == value\n",
        "        # Count occurrences per column.\n",
        "        counts_per_column = locations.sum()\n",
        "        # Filter to include only columns with one or more occurrences.\n",
        "        affected_columns = counts_per_column[counts_per_column > 0].to_dict()\n",
        "        # If any columns are affected, log the details.\n",
        "        if affected_columns:\n",
        "            replacement_audit[str(value)] = {\n",
        "                \"total_replacements\": int(locations.sum().sum()),\n",
        "                \"columns_affected\": affected_columns\n",
        "            }\n",
        "    # Perform the targeted replacement for string nulls.\n",
        "    df_cleaned[non_text_columns] = df_cleaned[non_text_columns].replace(string_nulls, np.nan)\n",
        "\n",
        "    # Store the complete replacement audit in the main audit log.\n",
        "    audit_log[\"data_quality_assessment\"][\"replacements_made\"] = replacement_audit\n",
        "\n",
        "    # Log the total number of NaN values per column after all cleansing operations.\n",
        "    audit_log[\"data_quality_assessment\"][\"nan_counts_post_cleansing\"] = \\\n",
        "        df_cleaned.isna().sum().to_dict()\n",
        "\n",
        "    # --- Step 2: Temporal Filtering Implementation (Unchanged) ---\n",
        "    # Log the original shape and date range of the DataFrame before filtering.\n",
        "    audit_log[\"temporal_filtering\"][\"original_row_count\"] = len(df_cleaned)\n",
        "    audit_log[\"temporal_filtering\"][\"original_date_range\"] = {\n",
        "        \"start\": df_cleaned.index.min().strftime('%Y-%m-%d'),\n",
        "        \"end\": df_cleaned.index.max().strftime('%Y-%m-%d')\n",
        "    }\n",
        "\n",
        "    # Extract the required date range from the configuration dictionary.\n",
        "    start_date_str = index_construction_config['corpus_parameters']['date_range_start']\n",
        "    end_date_str = index_construction_config['corpus_parameters']['date_range_end']\n",
        "\n",
        "    # Convert date strings to pandas Timestamp objects for robust filtering.\n",
        "    start_date = pd.to_datetime(start_date_str)\n",
        "    end_date = pd.to_datetime(end_date_str)\n",
        "\n",
        "    # Ensure the DataFrame index is sorted for efficient and correct slicing.\n",
        "    df_cleaned.sort_index(inplace=True)\n",
        "\n",
        "    # Apply the temporal filter using .loc for an inclusive date range slice.\n",
        "    df_filtered = df_cleaned.loc[start_date:end_date].copy()\n",
        "\n",
        "    # Log the new shape and date range after the filtering operation.\n",
        "    audit_log[\"temporal_filtering\"][\"filtered_row_count\"] = len(df_filtered)\n",
        "    audit_log[\"temporal_filtering\"][\"filtered_date_range\"] = {\n",
        "        \"start\": df_filtered.index.min().strftime('%Y-%m-%d'),\n",
        "        \"end\": df_filtered.index.max().strftime('%Y-%m-%d')\n",
        "    }\n",
        "\n",
        "    # --- Step 3: Document Count Verification (Unchanged) ---\n",
        "    # Extract the expected document count from the configuration.\n",
        "    expected_count = index_construction_config['corpus_parameters']['expected_document_count']\n",
        "\n",
        "    # Count the actual number of non-null EIU reports in the filtered data.\n",
        "    actual_count = df_filtered[text_column].count()\n",
        "\n",
        "    # Calculate the completion percentage of the text corpus.\n",
        "    completion_pct = actual_count / expected_count if expected_count > 0 else 0\n",
        "\n",
        "    # Generate the full expected monthly date range for the sample period.\n",
        "    expected_index = pd.date_range(\n",
        "        start=start_date, end=end_date, freq='MS'\n",
        "    )\n",
        "\n",
        "    # Identify the index of documents that are actually present and not null.\n",
        "    actual_index = df_filtered[df_filtered[text_column].notna()].index\n",
        "\n",
        "    # Find the difference between the expected and actual indices to get missing months.\n",
        "    missing_months = expected_index.difference(actual_index).strftime('%Y-%m').tolist()\n",
        "\n",
        "    # Log the detailed verification results.\n",
        "    verification_results = {\n",
        "        \"expected_document_count\": expected_count,\n",
        "        \"actual_document_count\": int(actual_count),\n",
        "        \"completion_percentage\": round(completion_pct, 4),\n",
        "        \"missing_document_months\": missing_months,\n",
        "        \"is_corpus_complete\": actual_count == expected_count\n",
        "    }\n",
        "    audit_log[\"document_count_verification\"] = verification_results\n",
        "\n",
        "    # Raise a critical error if the document coverage is below the specified threshold.\n",
        "    if completion_pct < missing_doc_threshold:\n",
        "        # The error message is specific and actionable for the user.\n",
        "        raise ValueError(\n",
        "            f\"EIU document coverage is {completion_pct:.2%}, which is below the \"\n",
        "            f\"required threshold of {missing_doc_threshold:.2%}. \"\n",
        "            f\"Missing {len(missing_months)} documents for months: {missing_months}\"\n",
        "        )\n",
        "\n",
        "    # Return the fully cleansed and filtered DataFrame and the detailed audit log.\n",
        "    return df_filtered, audit_log\n"
      ],
      "metadata": {
        "id": "O5f4PP5rmSzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Keyword List Preparation\n",
        "# Define a TypedDict for the structure of a single processed lexicon.\n",
        "# This enhances readability and allows for static type checking.\n",
        "class ProcessedLexicon(TypedDict):\n",
        "    unigrams: Set[str]\n",
        "    ngrams_by_first_token: Dict[str, List[Tuple[str, ...]]]\n",
        "    max_ngram_len: int\n",
        "\n",
        "# Define a TypedDict for the overall output structure.\n",
        "class PreparedLexicons(TypedDict):\n",
        "    uncertainty: ProcessedLexicon\n",
        "    brexit: ProcessedLexicon\n",
        "    covid: ProcessedLexicon\n",
        "    disambiguation_rules: List[Dict[str, Any]]\n",
        "\n",
        "def prepare_lexicons(\n",
        "    uncertainty_lexicon: List[str],\n",
        "    brexit_lexicon: List[str],\n",
        "    covid_lexicon: List[str]\n",
        ") -> PreparedLexicons:\n",
        "    \"\"\"\n",
        "    Processes and optimizes keyword lexicons for high-performance text matching.\n",
        "\n",
        "    This function transforms raw lists of keywords into a structured and highly\n",
        "    optimized format suitable for the Context-Aware Uncertainty Attribution\n",
        "    Algorithm. It performs three key steps:\n",
        "    1.  Normalizes all keywords (lowercase, strip whitespace) and separates them\n",
        "        into unigrams (single words) and n-grams (multi-word phrases).\n",
        "    2.  Implements the specific disambiguation rule from the research paper to\n",
        "        handle the \"Scottish referendum\" case by creating a machine-readable rule.\n",
        "    3.  Builds an optimized lookup structure for n-grams, indexed by their first\n",
        "        token and sorted by length (longest first), to enable efficient and\n",
        "        greedy matching within context windows.\n",
        "\n",
        "    Args:\n",
        "        uncertainty_lexicon (List[str]): The raw list of uncertainty keywords.\n",
        "        brexit_lexicon (List[str]): The raw list of Brexit-related keywords.\n",
        "        covid_lexicon (List[str]): The raw list of COVID-19 related keywords.\n",
        "\n",
        "    Returns:\n",
        "        PreparedLexicons: A TypedDict containing the processed and optimized\n",
        "            lexicons and any disambiguation rules. The structure is designed\n",
        "            for direct use in subsequent NLP tasks.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any of the input lexicons are empty, as this would\n",
        "                    indicate a critical configuration error.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure that lexicons are not empty, which would be a critical error.\n",
        "    if not all([uncertainty_lexicon, brexit_lexicon, covid_lexicon]):\n",
        "        raise ValueError(\"One or more input lexicons are empty.\")\n",
        "\n",
        "    # --- Helper Function for processing a single lexicon ---\n",
        "    def _process_single_lexicon(lexicon: List[str]) -> ProcessedLexicon:\n",
        "        \"\"\"Processes a single raw lexicon list into an optimized structure.\"\"\"\n",
        "        # Initialize containers for unigrams and n-grams.\n",
        "        unigrams: Set[str] = set()\n",
        "        # This dictionary will hold n-grams, keyed by their first word.\n",
        "        ngrams_by_first_token: Dict[str, List[Tuple[str, ...]]] = {}\n",
        "        # Track the maximum n-gram length for setting search bounds later.\n",
        "        max_ngram_len = 1\n",
        "\n",
        "        # --- Step 1: Multi-word Phrase Processing ---\n",
        "        # Iterate through each raw keyword in the provided list.\n",
        "        for raw_keyword in lexicon:\n",
        "            # Normalize the keyword: lowercase and strip leading/trailing whitespace.\n",
        "            # This ensures consistency with the text processing pipeline.\n",
        "            keyword = raw_keyword.lower().strip()\n",
        "\n",
        "            # Skip if the keyword becomes empty after stripping.\n",
        "            if not keyword:\n",
        "                continue\n",
        "\n",
        "            # Tokenize the keyword by splitting on one or more whitespace characters.\n",
        "            # This correctly handles phrases with multiple spaces.\n",
        "            tokens = tuple(re.split(r'\\s+', keyword))\n",
        "\n",
        "            # Classify as a unigram or n-gram based on the number of tokens.\n",
        "            if len(tokens) == 1:\n",
        "                # Add the single token to the set of unigrams for O(1) lookup.\n",
        "                unigrams.add(tokens[0])\n",
        "            else:\n",
        "                # This is a multi-word phrase (n-gram).\n",
        "                # Get the first token to use as a key in our lookup dictionary.\n",
        "                first_token = tokens[0]\n",
        "\n",
        "                # If this is the first time we see this starting token, initialize a list.\n",
        "                if first_token not in ngrams_by_first_token:\n",
        "                    ngrams_by_first_token[first_token] = []\n",
        "\n",
        "                # Append the tokenized phrase to the list for this starting token.\n",
        "                ngrams_by_first_token[first_token].append(tokens)\n",
        "\n",
        "                # Update the maximum n-gram length found in this lexicon.\n",
        "                if len(tokens) > max_ngram_len:\n",
        "                    max_ngram_len = len(tokens)\n",
        "\n",
        "        # --- Step 3: Lexicon Optimization for Context Window Processing ---\n",
        "        # For each starting token, sort its associated n-grams by length in\n",
        "        # descending order. This is crucial for implementing a \"greedy\" matching\n",
        "        # strategy that finds the longest possible phrase first.\n",
        "        # E.g., it will match \"exiting the european union\" before \"exiting the eu\".\n",
        "        for first_token in ngrams_by_first_token:\n",
        "            ngrams_by_first_token[first_token].sort(key=len, reverse=True)\n",
        "\n",
        "        # Return the fully processed and optimized lexicon structure.\n",
        "        return {\n",
        "            \"unigrams\": unigrams,\n",
        "            \"ngrams_by_first_token\": ngrams_by_first_token,\n",
        "            \"max_ngram_len\": max_ngram_len\n",
        "        }\n",
        "\n",
        "    # --- Main Function Logic ---\n",
        "    # Process each of the three main lexicons using the helper function.\n",
        "    processed_uncertainty = _process_single_lexicon(uncertainty_lexicon)\n",
        "    processed_brexit = _process_single_lexicon(brexit_lexicon)\n",
        "    processed_covid = _process_single_lexicon(covid_lexicon)\n",
        "\n",
        "    # --- Step 2: Disambiguation Rule Implementation ---\n",
        "    # As per Table 1, Note 2: \"any context containing 'referendum' alongside\n",
        "    # 'Scotland' or 'Scottish' was excluded from the analysis\".\n",
        "    # This is encoded as a machine-readable rule for the downstream processor.\n",
        "    disambiguation_rules = [\n",
        "        {\n",
        "            \"target_keyword\": \"referendum\",\n",
        "            \"exclusion_keywords\": {\"scotland\", \"scottish\"},\n",
        "            \"applies_to_lexicon\": \"brexit\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Assemble the final, comprehensive `PreparedLexicons` object.\n",
        "    # This structure is self-contained and ready for the text analysis pipeline.\n",
        "    prepared_lexicons: PreparedLexicons = {\n",
        "        \"uncertainty\": processed_uncertainty,\n",
        "        \"brexit\": processed_brexit,\n",
        "        \"covid\": processed_covid,\n",
        "        \"disambiguation_rules\": disambiguation_rules\n",
        "    }\n",
        "\n",
        "    return prepared_lexicons\n"
      ],
      "metadata": {
        "id": "5gi4dWOtoRZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Text Preprocessing\n",
        "\n",
        "class ProcessedLexicon(TypedDict):\n",
        "    unigrams: Set[str]\n",
        "    ngrams_by_first_token: Dict[str, List[Tuple[str, ...]]]\n",
        "    max_ngram_len: int\n",
        "\n",
        "class PreparedLexicons(TypedDict):\n",
        "    uncertainty: ProcessedLexicon\n",
        "    brexit: ProcessedLexicon\n",
        "    covid: ProcessedLexicon\n",
        "    disambiguation_rules: List[Dict[str, Any]]\n",
        "\n",
        "def preprocess_text_corpus(\n",
        "    df: pd.DataFrame,\n",
        "    prepared_lexicons: PreparedLexicons,\n",
        "    language: str = 'english'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies a sequential, configurable NLP preprocessing pipeline to a text corpus.\n",
        "\n",
        "    This function executes the text processing pipeline as defined in Steps 3-6\n",
        "    of the research methodology. It takes a cleansed DataFrame and prepared\n",
        "    lexicons, and returns a DataFrame augmented with new columns representing\n",
        "    each stage of preprocessing. This ensures a fully traceable and auditable\n",
        "    data transformation process.\n",
        "\n",
        "    The pipeline includes:\n",
        "    1.  Unicode and Lowercase Normalization: Standardizes text for matching.\n",
        "    2.  NLTK Tokenization: Splits text into words and punctuation.\n",
        "    3.  Stopword Removal: Removes common function words for a specified language,\n",
        "        while preserving any keywords from the lexicons.\n",
        "    4.  N-gram Generation: Creates bigram and trigram sequences.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The cleansed DataFrame from Task 1. Must contain an\n",
        "            'EIU' column with the raw text.\n",
        "        prepared_lexicons (PreparedLexicons): The optimized lexicon structure\n",
        "            from Task 2, used to prevent keyword removal.\n",
        "        language (str): The language for stopword removal, corresponding to\n",
        "            NLTK's available corpus languages. Defaults to 'english'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The input DataFrame augmented with the following new\n",
        "            columns: 'EIU_lowercase', 'EIU_tokens', 'EIU_cleaned_tokens',\n",
        "            'bigrams', and 'trigrams'.\n",
        "\n",
        "    Raises:\n",
        "        LookupError: If required NLTK data packages (e.g., 'punkt', 'stopwords')\n",
        "            for the specified language are not found on the system.\n",
        "        KeyError: If the input DataFrame is missing the required 'EIU' column.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that the essential 'EIU' column exists in the DataFrame.\n",
        "    if 'EIU' not in df.columns:\n",
        "        # Raise an error if the required column is not found.\n",
        "        raise KeyError(\"Input DataFrame must contain an 'EIU' column.\")\n",
        "\n",
        "    # Create a deep copy of the DataFrame to ensure the original is not modified.\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # --- Step 1: Lowercase Normalization (Methodology Step 3) ---\n",
        "    # Define a helper function for robust text normalization.\n",
        "    def normalize_text(text: Any) -> Any:\n",
        "        \"\"\"Applies Unicode normalization and lowercasing, handling NaNs.\"\"\"\n",
        "        # Check if the input is a valid string; otherwise, return it as is (e.g., NaN).\n",
        "        if not isinstance(text, str):\n",
        "            return text\n",
        "        # Apply NFKC Unicode normalization to handle compatibility characters (e.g., ligatures).\n",
        "        normalized_text = unicodedata.normalize('NFKC', text)\n",
        "        # Convert the entire normalized string to lowercase for case-insensitive matching.\n",
        "        return normalized_text.lower()\n",
        "\n",
        "    # Apply the normalization function to the 'EIU' column using pandas .apply method.\n",
        "    df_processed['EIU_lowercase'] = df_processed['EIU'].apply(normalize_text)\n",
        "\n",
        "    # --- Step 2: NLTK Tokenization (Methodology Step 4) ---\n",
        "    # Define a helper function for tokenization with error handling.\n",
        "    def tokenize_text(text: Any) -> List[str]:\n",
        "        \"\"\"Tokenizes text using NLTK, handling NaNs and missing data errors.\"\"\"\n",
        "        # Return an empty list for non-string inputs (e.g., NaN), representing zero tokens.\n",
        "        if not isinstance(text, str):\n",
        "            return []\n",
        "        # Use a try-except block to handle cases where NLTK data is not downloaded.\n",
        "        try:\n",
        "            # Use the NLTK word tokenizer as specified in the methodology.\n",
        "            return nltk.word_tokenize(text)\n",
        "        # Catch the specific error for a missing NLTK data package.\n",
        "        except LookupError:\n",
        "            # Raise a new, more informative error with an actionable instruction.\n",
        "            raise LookupError(\n",
        "                \"NLTK 'punkt' tokenizer data not found. Please run: \"\n",
        "                \"import nltk; nltk.download('punkt')\"\n",
        "            )\n",
        "\n",
        "    # Apply the tokenization function to the newly created lowercase column.\n",
        "    df_processed['EIU_tokens'] = df_processed['EIU_lowercase'].apply(tokenize_text)\n",
        "\n",
        "    # --- Step 3: Stopword Removal (Methodology Step 5) ---\n",
        "    # Use a try-except block for robustly fetching the stopwords list.\n",
        "    try:\n",
        "        # Remediation: Fetch the stopword list for the specified language parameter.\n",
        "        stopwords_base = set(nltk.corpus.stopwords.words(language))\n",
        "    # Catch the specific error for a missing NLTK data package.\n",
        "    except LookupError:\n",
        "        # Remediation: Raise a dynamic error message that includes the requested language.\n",
        "        raise LookupError(\n",
        "            f\"NLTK 'stopwords' data for language '{language}' not found. \"\n",
        "            \"Please run: import nltk; nltk.download('stopwords')\"\n",
        "        )\n",
        "\n",
        "    # --- Advanced Stopword Handling: Preserve Lexicon Keywords ---\n",
        "    # Aggregate all unigram keywords from all prepared lexicons into a single set.\n",
        "    all_lexicon_unigrams = (\n",
        "        prepared_lexicons['uncertainty']['unigrams'] |\n",
        "        prepared_lexicons['brexit']['unigrams'] |\n",
        "        prepared_lexicons['covid']['unigrams']\n",
        "    )\n",
        "    # Create the final stopword set by removing any words that also appear in our lexicons.\n",
        "    final_stopwords = stopwords_base - all_lexicon_unigrams\n",
        "\n",
        "    # Define a helper function for the stopword removal process.\n",
        "    def remove_stopwords(tokens: List[str]) -> List[str]:\n",
        "        \"\"\"Removes stopwords from a list of tokens using the final stopword set.\"\"\"\n",
        "        # Use a list comprehension for an efficient filter operation.\n",
        "        return [token for token in tokens if token not in final_stopwords]\n",
        "\n",
        "    # Apply the stopword removal function to the tokenized column.\n",
        "    df_processed['EIU_cleaned_tokens'] = df_processed['EIU_tokens'].apply(remove_stopwords)\n",
        "\n",
        "    # --- Step 4: N-gram Analysis (Methodology Step 6) ---\n",
        "    # Define a helper function to generate n-grams robustly.\n",
        "    def generate_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
        "        \"\"\"Generates n-grams from a list of tokens, handling lists shorter than n.\"\"\"\n",
        "        # N-grams can only be generated if the number of tokens is at least n.\n",
        "        if len(tokens) < n:\n",
        "            # Return an empty list if not enough tokens are available.\n",
        "            return []\n",
        "        # Use the NLTK bigrams function if n is 2.\n",
        "        if n == 2:\n",
        "            return list(nltk.bigrams(tokens))\n",
        "        # Use the NLTK trigrams function if n is 3.\n",
        "        elif n == 3:\n",
        "            return list(nltk.trigrams(tokens))\n",
        "        # Return an empty list for other values of n.\n",
        "        return []\n",
        "\n",
        "    # Apply the n-gram generation function to create a 'bigrams' column.\n",
        "    df_processed['bigrams'] = df_processed['EIU_cleaned_tokens'].apply(\n",
        "        lambda tokens: generate_ngrams(tokens, 2)\n",
        "    )\n",
        "    # Apply the n-gram generation function to create a 'trigrams' column.\n",
        "    df_processed['trigrams'] = df_processed['EIU_cleaned_tokens'].apply(\n",
        "        lambda tokens: generate_ngrams(tokens, 3)\n",
        "    )\n",
        "\n",
        "    # Return the fully augmented DataFrame with all preprocessing columns.\n",
        "    return df_processed\n"
      ],
      "metadata": {
        "id": "vDgwk11zqhW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Named Entity Recognition\n",
        "# Define a TypedDict for the structure of a single extracted entity.\n",
        "# This provides clarity and enables static analysis of the output.\n",
        "class NerEntity(TypedDict):\n",
        "    text: str\n",
        "    label: str\n",
        "    start_char: int\n",
        "    end_char: int\n",
        "\n",
        "def load_spacy_model_for_ner(model_name: str = \"en_core_web_lg\") -> Language:\n",
        "    \"\"\"\n",
        "    Loads the specified SpaCy model, optimized for Named Entity Recognition.\n",
        "\n",
        "    This function loads a SpaCy language model and disables all pipeline\n",
        "    components except for the Named Entity Recognizer ('ner'). This is a\n",
        "    critical performance optimization that focuses computational resources on\n",
        "    the required task. It includes robust error handling for cases where the\n",
        "    model is not installed.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name of the SpaCy model to load. Defaults to\n",
        "            \"en_core_web_lg\" as specified in the research methodology.\n",
        "\n",
        "    Returns:\n",
        "        Language: The loaded and optimized SpaCy Language object.\n",
        "\n",
        "    Raises:\n",
        "        OSError: If the specified SpaCy model is not found on the system,\n",
        "            providing a clear installation instruction.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the specified SpaCy model, disabling all components not needed for NER.\n",
        "        # This significantly speeds up processing and reduces memory usage.\n",
        "        nlp = spacy.load(\n",
        "            model_name,\n",
        "            disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]\n",
        "        )\n",
        "        return nlp\n",
        "    except OSError:\n",
        "        # Catch the specific error for a missing model and raise a more\n",
        "        # informative exception with actionable advice for the user.\n",
        "        error_message = (\n",
        "            f\"SpaCy model '{model_name}' not found. Please install it by \"\n",
        "            f\"running the following command in your terminal:\\n\"\n",
        "            f\"python -m spacy download {model_name}\"\n",
        "        )\n",
        "        raise OSError(error_message)\n",
        "\n",
        "def apply_ner_to_corpus(\n",
        "    df: pd.DataFrame,\n",
        "    nlp: Language,\n",
        "    text_column: str = \"EIU_lowercase\",\n",
        "    batch_size: int = 50\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies Named Entity Recognition to a corpus of texts using batch processing.\n",
        "\n",
        "    This function uses a pre-loaded, optimized SpaCy model to perform NER on\n",
        "    an entire column of text from a DataFrame. It leverages SpaCy's `nlp.pipe`\n",
        "    for efficient, memory-safe batch processing. It also dynamically adjusts\n",
        "    the model's `max_length` to handle potentially very long documents without\n",
        "    error.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the text data. It is\n",
        "            assumed to have passed through the preprocessing steps of Task 3.\n",
        "        nlp (Language): The pre-loaded and optimized SpaCy Language object from\n",
        "            `load_spacy_model_for_ner`.\n",
        "        text_column (str): The name of the column containing the normalized\n",
        "            text to be processed. Defaults to \"EIU_lowercase\".\n",
        "        batch_size (int): The number of documents to process in each batch.\n",
        "            Tuning this can affect the trade-off between speed and memory usage.\n",
        "            Defaults to 50.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The input DataFrame augmented with a new 'ner_entities'\n",
        "            column. Each entry in this column is a list of dictionaries, where\n",
        "            each dictionary represents a single named entity.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the specified `text_column` does not exist in the DataFrame.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that the specified text column exists.\n",
        "    if text_column not in df.columns:\n",
        "        raise KeyError(f\"The specified text column '{text_column}' was not found in the DataFrame.\")\n",
        "\n",
        "    # Create a deep copy to avoid side effects on the original DataFrame.\n",
        "    df_ner = df.copy()\n",
        "\n",
        "    # --- Step 2: Batch Processing Implementation ---\n",
        "    # Convert the text column to a list, replacing any NaN values with empty\n",
        "    # strings to ensure nlp.pipe receives a clean list of strings.\n",
        "    texts = df_ner[text_column].fillna('').tolist()\n",
        "\n",
        "    # --- Robustness: Handle potentially long documents ---\n",
        "    # Calculate the maximum document length in the corpus.\n",
        "    max_len = max(len(text) for text in texts) if texts else 0\n",
        "    # If the longest document exceeds the model's default max_length, increase it.\n",
        "    # This prevents a ValueError for long texts. Add a small buffer.\n",
        "    if max_len > nlp.max_length:\n",
        "        nlp.max_length = max_len + 100\n",
        "\n",
        "    # Initialize a list to store the extracted entities for each document.\n",
        "    all_entities: List[List[NerEntity]] = []\n",
        "\n",
        "    # Process the texts using nlp.pipe for efficient batching.\n",
        "    # This is significantly faster and more memory-efficient than a simple loop.\n",
        "    # The `as_tuples=True` argument can further optimize if only text and doc are needed,\n",
        "    # but here we process the full doc object to access .ents.\n",
        "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
        "        # For each processed document, extract the entities.\n",
        "        # Create a list of structured dictionaries for the current document's entities.\n",
        "        doc_entities: List[NerEntity] = [\n",
        "            {\n",
        "                \"text\": ent.text,\n",
        "                \"label\": ent.label_,\n",
        "                \"start_char\": ent.start_char,\n",
        "                \"end_char\": ent.end_char\n",
        "            }\n",
        "            for ent in doc.ents\n",
        "        ]\n",
        "        # Append the list of entities for this document to the main list.\n",
        "        all_entities.append(doc_entities)\n",
        "\n",
        "    # Add the list of extracted entities as a new column to the DataFrame.\n",
        "    # The list `all_entities` has the same order and length as the input `texts`\n",
        "    # list, ensuring perfect alignment with the DataFrame's index.\n",
        "    df_ner['ner_entities'] = all_entities\n",
        "\n",
        "    # --- Step 3: Entity Integration Strategy ---\n",
        "    # The integration of these entities is a strategic choice for the next task.\n",
        "    # This function's role is to provide the structured data. The downstream\n",
        "    # function (Task 5) will use this 'ner_entities' column to dynamically\n",
        "    # augment its keyword search within context windows, checking for the text\n",
        "    # of entities with relevant labels (e.g., ORG, GPE, EVENT).\n",
        "\n",
        "    return df_ner\n"
      ],
      "metadata": {
        "id": "ms0MTJ9Gs2BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Context-Aware Uncertainty Attribution\n",
        "\n",
        "class ProcessedLexicon(TypedDict):\n",
        "    unigrams: Set[str]\n",
        "    ngrams_by_first_token: Dict[str, List[Tuple[str, ...]]]\n",
        "    max_ngram_len: int\n",
        "\n",
        "class PreparedLexicons(TypedDict):\n",
        "    uncertainty: ProcessedLexicon\n",
        "    brexit: ProcessedLexicon\n",
        "    covid: ProcessedLexicon\n",
        "    disambiguation_rules: List[Dict[str, Any]]\n",
        "\n",
        "def _find_keywords_in_window(\n",
        "    window: List[str],\n",
        "    lexicon: ProcessedLexicon\n",
        ") -> Set[str]:\n",
        "    \"\"\"\n",
        "    Finds all matching unigram and n-gram keywords from a lexicon in a token window.\n",
        "    This is a high-performance helper using pre-computed lexicon structures.\n",
        "    \"\"\"\n",
        "    found_keywords: Set[str] = set()\n",
        "    i = 0\n",
        "    while i < len(window):\n",
        "        token = window[i]\n",
        "        matched = False\n",
        "        # Check for n-gram matches first (greedy, longest-first approach).\n",
        "        if token in lexicon['ngrams_by_first_token']:\n",
        "            for ngram in lexicon['ngrams_by_first_token'][token]:\n",
        "                # Check if the n-gram fits within the rest of the window.\n",
        "                if i + len(ngram) <= len(window):\n",
        "                    # Compare the slice of the window with the n-gram tuple.\n",
        "                    if tuple(window[i : i + len(ngram)]) == ngram:\n",
        "                        # Add the matched phrase (joined back to a string).\n",
        "                        found_keywords.add(\" \".join(ngram))\n",
        "                        # Advance the index by the length of the matched n-gram.\n",
        "                        i += len(ngram)\n",
        "                        matched = True\n",
        "                        # Break after the first (longest) match for this position.\n",
        "                        break\n",
        "        # If no n-gram was matched at this position, check for a unigram.\n",
        "        if not matched:\n",
        "            if token in lexicon['unigrams']:\n",
        "                found_keywords.add(token)\n",
        "            # Advance the index by one.\n",
        "            i += 1\n",
        "    return found_keywords\n",
        "\n",
        "def attribute_uncertainty_in_corpus(\n",
        "    df: pd.DataFrame,\n",
        "    prepared_lexicons: PreparedLexicons,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Executes the Context-Aware Uncertainty Attribution algorithm on the corpus.\n",
        "\n",
        "    This function is the core of the BRUI calculation. It iterates through each\n",
        "    document's cleaned tokens, identifies every instance of an uncertainty\n",
        "    keyword, and analyzes its surrounding context window (as per Equation 1).\n",
        "    For each window, it detects the presence of Brexit and COVID-19 keywords\n",
        "    and classifies the uncertainty accordingly. The final output is a DataFrame\n",
        "    augmented with the raw counts needed for the index construction in Task 6.\n",
        "\n",
        "    The process for each document is:\n",
        "    1.  Iterate through tokens to find uncertainty keywords.\n",
        "    2.  For each uncertainty keyword, construct the 10-word context window.\n",
        "    3.  Search the window for Brexit and COVID-19 keywords using an optimized\n",
        "        matching algorithm that handles n-grams and disambiguation rules.\n",
        "    4.  Classify the uncertainty instance based on the findings (Pure Brexit,\n",
        "        Pure COVID, or Mixed) and increment the appropriate counter.\n",
        "    5.  Store the final counts for the document.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The preprocessed DataFrame from Task 3, containing\n",
        "            the 'EIU_cleaned_tokens' column.\n",
        "        prepared_lexicons (PreparedLexicons): The optimized lexicon structure\n",
        "            from Task 2.\n",
        "        config (Dict[str, Any]): The main `index_construction_config` dictionary,\n",
        "            used to get the context window size.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The input DataFrame augmented with three new columns:\n",
        "            'pure_brexit_count', 'pure_covid_count', and 'mixed_count',\n",
        "            containing the raw attribution counts for each document.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If required columns or configuration keys are missing.\n",
        "    \"\"\"\n",
        "    # --- Input Validation and Setup ---\n",
        "    required_column = 'EIU_cleaned_tokens'\n",
        "    if required_column not in df.columns:\n",
        "        raise KeyError(f\"Input DataFrame must contain the '{required_column}' column.\")\n",
        "\n",
        "    # Create a deep copy to avoid modifying the original DataFrame.\n",
        "    df_attributed = df.copy()\n",
        "\n",
        "    # Extract algorithm parameters from the configuration.\n",
        "    try:\n",
        "        window_size = config['algorithm_parameters']['context_window_size']\n",
        "    except KeyError as e:\n",
        "        raise KeyError(f\"Missing required configuration key: {e}\")\n",
        "\n",
        "    # Extract the specific lexicons for easier access.\n",
        "    uncertainty_lex = prepared_lexicons['uncertainty']\n",
        "    brexit_lex = prepared_lexicons['brexit']\n",
        "    covid_lex = prepared_lexicons['covid']\n",
        "\n",
        "    # Extract disambiguation rule details.\n",
        "    # This makes the code cleaner and assumes one rule for now as per the paper.\n",
        "    disambiguation_rule = prepared_lexicons['disambiguation_rules'][0]\n",
        "    dis_target = disambiguation_rule['target_keyword']\n",
        "    dis_exclusions = disambiguation_rule['exclusion_keywords']\n",
        "\n",
        "    # Initialize lists to store the results for each document.\n",
        "    pure_brexit_counts: List[int] = []\n",
        "    pure_covid_counts: List[int] = []\n",
        "    mixed_counts: List[int] = []\n",
        "\n",
        "    # --- Main Loop: Iterate through each document (row) in the DataFrame ---\n",
        "    for tokens in df_attributed[required_column]:\n",
        "        # Initialize counters for the current document.\n",
        "        doc_pure_brexit = 0\n",
        "        doc_pure_covid = 0\n",
        "        doc_mixed = 0\n",
        "\n",
        "        # If the document has no tokens, append 0 counts and continue.\n",
        "        if not tokens:\n",
        "            pure_brexit_counts.append(0)\n",
        "            pure_covid_counts.append(0)\n",
        "            mixed_counts.append(0)\n",
        "            continue\n",
        "\n",
        "        # --- Step 1: Context Window Construction (based on finding U) ---\n",
        "        # Iterate through each token's index to check for uncertainty keywords.\n",
        "        for i, token in enumerate(tokens):\n",
        "            # Check if the current token is an uncertainty unigram or starts an n-gram.\n",
        "            is_uncertainty_keyword = (\n",
        "                token in uncertainty_lex['unigrams'] or\n",
        "                token in uncertainty_lex['ngrams_by_first_token']\n",
        "            )\n",
        "\n",
        "            if is_uncertainty_keyword:\n",
        "                # An uncertainty keyword 'U' is found. Construct the context window.\n",
        "                # Equation 1: CW = {x_{-10}, ..., U, ..., x_{+10}}\n",
        "                start = max(0, i - window_size)\n",
        "                end = i + window_size + 1 # Slice end is exclusive.\n",
        "                context_window = tokens[start:end]\n",
        "\n",
        "                # --- Step 2: Keyword Detection within the Window ---\n",
        "                # Find all Brexit and COVID keywords present in the window.\n",
        "                brexit_words_found = _find_keywords_in_window(context_window, brexit_lex)\n",
        "                covid_words_found = _find_keywords_in_window(context_window, covid_lex)\n",
        "\n",
        "                # Apply the \"Scottish referendum\" disambiguation rule.\n",
        "                # If 'referendum' was found, check if any exclusion words were also found.\n",
        "                if dis_target in brexit_words_found:\n",
        "                    # Check for intersection between found words and exclusion words.\n",
        "                    if not brexit_words_found.isdisjoint(dis_exclusions):\n",
        "                        # If there's an overlap, remove 'referendum' from the set\n",
        "                        # of found words, effectively ignoring it for this window.\n",
        "                        brexit_words_found.remove(dis_target)\n",
        "\n",
        "                # Determine the final boolean flags after disambiguation.\n",
        "                brexit_found = bool(brexit_words_found)\n",
        "                covid_found = bool(covid_words_found)\n",
        "\n",
        "                # --- Step 3: Conditional Logic Implementation (Equation 2) ---\n",
        "                # Classify the uncertainty instance and increment the correct counter.\n",
        "                if brexit_found and not covid_found:\n",
        "                    # Case 1: Pure Brexit-related uncertainty.\n",
        "                    doc_pure_brexit += 1\n",
        "                elif covid_found and not brexit_found:\n",
        "                    # Case 2: Pure COVID-related uncertainty.\n",
        "                    doc_pure_covid += 1\n",
        "                elif brexit_found and covid_found:\n",
        "                    # Case 3: Mixed uncertainty, to be allocated proportionally later.\n",
        "                    doc_mixed += 1\n",
        "\n",
        "        # After processing all tokens in the document, append the final counts.\n",
        "        pure_brexit_counts.append(doc_pure_brexit)\n",
        "        pure_covid_counts.append(doc_pure_covid)\n",
        "        mixed_counts.append(doc_mixed)\n",
        "\n",
        "    # Add the new count columns to the DataFrame.\n",
        "    df_attributed['pure_brexit_count'] = pure_brexit_counts\n",
        "    df_attributed['pure_covid_count'] = pure_covid_counts\n",
        "    df_attributed['mixed_count'] = mixed_counts\n",
        "\n",
        "    return df_attributed\n",
        "\n"
      ],
      "metadata": {
        "id": "faKuF0jBvAF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Brexit-Related Uncertainty Index (BRUI) Calculation\n",
        "\n",
        "def calculate_brui(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates the final Brexit-Related Uncertainty Index (BRUI).\n",
        "\n",
        "    This function executes the final four steps of the index construction\n",
        "    methodology, transforming the raw attribution counts into the standardized\n",
        "    and normalized BRUI. It operates with high numerical precision and directly\n",
        "    implements the specified equations and logic from the research paper.\n",
        "\n",
        "    The calculation pipeline is as follows:\n",
        "    1.  TBRUKN Aggregation: Calculates the Total Brexit-Related Uncertainty\n",
        "        Keyword Number by applying proportional allocation to mixed-count contexts.\n",
        "    2.  Total Word Count: Computes the total number of cleaned tokens for each\n",
        "        document to be used as a normalization factor.\n",
        "    3.  Standardization (Equation 3): Calculates the raw uncertainty density\n",
        "        (BRUI_raw) by dividing the TBRUKN by the total word count.\n",
        "    4.  Max-Normalization (Step 11): Scales the raw index so that its maximum\n",
        "        value over the entire period is 100.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame from Task 5, containing the columns\n",
        "            'pure_brexit_count', 'pure_covid_count', 'mixed_count', and\n",
        "            'EIU_cleaned_tokens'.\n",
        "        config (Dict[str, Any]): The main `index_construction_config` dictionary,\n",
        "            used to get the fallback weight for proportional allocation.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The input DataFrame augmented with the final 'BRUI' column\n",
        "            as well as intermediate calculation columns ('TBRUKN',\n",
        "            'total_word_count', 'BRUI_raw') for full auditability.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If required columns or configuration keys are missing.\n",
        "    \"\"\"\n",
        "    # --- Input Validation and Setup ---\n",
        "    required_cols = [\n",
        "        'pure_brexit_count', 'pure_covid_count', 'mixed_count', 'EIU_cleaned_tokens'\n",
        "    ]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        raise KeyError(f\"Input DataFrame is missing one or more required columns: {required_cols}\")\n",
        "\n",
        "    # Create a deep copy to avoid modifying the original DataFrame.\n",
        "    df_final = df.copy()\n",
        "\n",
        "    # Extract the fallback weight from the configuration for proportional allocation.\n",
        "    try:\n",
        "        fallback_weight = config['algorithm_parameters']['mixed_count_allocation']['fallback']\n",
        "    except KeyError as e:\n",
        "        raise KeyError(f\"Missing required configuration key for fallback weight: {e}\")\n",
        "\n",
        "    # --- Step 1: Total Brexit Related Uncertainty Keyword Number (TBRUKN) Aggregation ---\n",
        "    # This step implements the proportional allocation for mixed-uncertainty contexts.\n",
        "\n",
        "    # Calculate the denominator for the weight calculation: pure_brexit + pure_covid.\n",
        "    pure_sum = df_final['pure_brexit_count'] + df_final['pure_covid_count']\n",
        "\n",
        "    # Calculate the Brexit weight. Use the fallback where the sum of pure counts is zero.\n",
        "    # brexit_weight = pure_brexit_count / (pure_brexit_count + pure_covid_count)\n",
        "    brexit_weight = np.divide(\n",
        "        df_final['pure_brexit_count'],\n",
        "        pure_sum,\n",
        "        out=np.full_like(pure_sum, fill_value=fallback_weight, dtype=float),\n",
        "        where=(pure_sum != 0)\n",
        "    )\n",
        "\n",
        "    # Calculate the final TBRUKN for each month (t).\n",
        "    # TBRUKN_t = pure_brexit_count_t + (mixed_count_t * brexit_proportion_weight_t)\n",
        "    df_final['TBRUKN'] = df_final['pure_brexit_count'] + (df_final['mixed_count'] * brexit_weight)\n",
        "\n",
        "    # --- Step 2: Total Word Count Calculation for Standardization ---\n",
        "    # The word count must be based on the cleaned tokens to match the analysis basis.\n",
        "    df_final['total_word_count'] = df_final['EIU_cleaned_tokens'].apply(len)\n",
        "\n",
        "    # --- Step 3: BRUI Standardization Implementation (Equation 3) ---\n",
        "    # This step calculates the raw uncertainty density for each document.\n",
        "    # BRUI_t = TBRUKN_t / (Total Number of Words Per Report)_t\n",
        "\n",
        "    # Perform safe division, returning 0.0 where the word count is 0.\n",
        "    df_final['BRUI_raw'] = np.divide(\n",
        "        df_final['TBRUKN'],\n",
        "        df_final['total_word_count'],\n",
        "        out=np.zeros_like(df_final['TBRUKN'], dtype=float),\n",
        "        where=(df_final['total_word_count'] != 0)\n",
        "    )\n",
        "\n",
        "    # --- Step 4: Max-Normalization Implementation (Methodology Step 11) ---\n",
        "    # This step scales the entire series so that the peak uncertainty month is 100.\n",
        "\n",
        "    # Find the maximum value of the raw, standardized BRUI series.\n",
        "    max_raw_brui = df_final['BRUI_raw'].max()\n",
        "\n",
        "    # Get the normalization target from the configuration.\n",
        "    norm_target = config['finalization_parameters']['normalization_target_max']\n",
        "\n",
        "    # Normalize the series. If the max value is 0, the result is an all-zero series.\n",
        "    # normalized_BRUI = (BRUI_raw / max(BRUI_raw)) * 100\n",
        "    if max_raw_brui > 0:\n",
        "        df_final['BRUI'] = (df_final['BRUI_raw'] / max_raw_brui) * norm_target\n",
        "    else:\n",
        "        # If no uncertainty was ever detected, the index is zero everywhere.\n",
        "        df_final['BRUI'] = 0.0\n",
        "\n",
        "    return df_final\n"
      ],
      "metadata": {
        "id": "VErY4tHiwIoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: COVID-19 Related Uncertainty Index (CRUI) Calculation\n",
        "\n",
        "def calculate_crui(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates the final COVID-19 Related Uncertainty Index (CRUI).\n",
        "\n",
        "    This function constructs the CRUI by applying the identical, symmetric\n",
        "    methodology used for the BRUI. It leverages the same raw attribution counts\n",
        "    ('pure_brexit_count', 'pure_covid_count', 'mixed_count') to ensure perfect\n",
        "    consistency in the disentanglement of the two uncertainty sources.\n",
        "\n",
        "    The calculation pipeline mirrors the BRUI construction:\n",
        "    1.  TCRUKN Aggregation: Calculates the Total COVID-19 Related Uncertainty\n",
        "        Keyword Number using a proportional allocation symmetric to the BRUI's.\n",
        "    2.  Standardization: Calculates the raw CRUI density by dividing the TCRUKN\n",
        "        by the same total word count used for the BRUI.\n",
        "    3.  Max-Normalization: Scales the raw CRUI series independently so that its\n",
        "        own maximum value over the entire period is 100.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame from Task 6, which must contain the\n",
        "            raw attribution counts and the 'total_word_count' column.\n",
        "        config (Dict[str, Any]): The main `index_construction_config` dictionary,\n",
        "            used to get the fallback weight for proportional allocation.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The input DataFrame augmented with the final 'CRUI' column\n",
        "            and its intermediate calculation columns ('TCRUKN', 'CRUI_raw')\n",
        "            for full auditability.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If required columns from previous tasks or configuration\n",
        "            keys are missing.\n",
        "    \"\"\"\n",
        "    # --- Input Validation and Setup ---\n",
        "    # Verify that all required columns from previous tasks are present.\n",
        "    required_cols = [\n",
        "        'pure_brexit_count', 'pure_covid_count', 'mixed_count', 'total_word_count'\n",
        "    ]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        raise KeyError(f\"Input DataFrame is missing one or more required columns: {required_cols}\")\n",
        "\n",
        "    # Create a deep copy to avoid modifying the original DataFrame.\n",
        "    df_final = df.copy()\n",
        "\n",
        "    # Extract the fallback weight from the configuration. This must be the same\n",
        "    # as used for the BRUI to ensure consistency.\n",
        "    try:\n",
        "        fallback_weight = config['algorithm_parameters']['mixed_count_allocation']['fallback']\n",
        "    except KeyError as e:\n",
        "        raise KeyError(f\"Missing required configuration key for fallback weight: {e}\")\n",
        "\n",
        "    # --- Step 1 & 2: TCRUKN Calculation with Proportional Allocation Consistency ---\n",
        "    # This step calculates the Total COVID-19 Related Uncertainty Keyword Number.\n",
        "\n",
        "    # The denominator is the same as for the BRUI calculation.\n",
        "    pure_sum = df_final['pure_brexit_count'] + df_final['pure_covid_count']\n",
        "\n",
        "    # Calculate the COVID weight, which is the symmetric counterpart to the Brexit weight.\n",
        "    # covid_weight = pure_covid_count / (pure_brexit_count + pure_covid_count)\n",
        "    covid_weight = np.divide(\n",
        "        df_final['pure_covid_count'],\n",
        "        pure_sum,\n",
        "        out=np.full_like(pure_sum, fill_value=fallback_weight, dtype=float),\n",
        "        where=(pure_sum != 0)\n",
        "    )\n",
        "\n",
        "    # Calculate the final TCRUKN for each month (t).\n",
        "    # TCRUKN_t = pure_covid_count_t + (mixed_count_t * covid_proportion_weight_t)\n",
        "    df_final['TCRUKN'] = df_final['pure_covid_count'] + (df_final['mixed_count'] * covid_weight)\n",
        "\n",
        "    # --- Step 3: CRUI Standardization and Normalization ---\n",
        "    # The process is identical to the BRUI but uses the TCRUKN as the numerator.\n",
        "\n",
        "    # Standardization: Calculate the raw CRUI density.\n",
        "    # CRUI_raw_t = TCRUKN_t / (Total Number of Words Per Report)_t\n",
        "    # We reuse the 'total_word_count' column calculated for the BRUI to ensure\n",
        "    # the normalization base is identical, which is critical for comparing densities.\n",
        "    df_final['CRUI_raw'] = np.divide(\n",
        "        df_final['TCRUKN'],\n",
        "        df_final['total_word_count'],\n",
        "        out=np.zeros_like(df_final['TCRUKN'], dtype=float),\n",
        "        where=(df_final['total_word_count'] != 0)\n",
        "    )\n",
        "\n",
        "    # Normalization: Scale the CRUI series to its own maximum value of 100.\n",
        "    # This is done independently of the BRUI's normalization.\n",
        "\n",
        "    # Find the maximum value of the raw, standardized CRUI series.\n",
        "    max_raw_crui = df_final['CRUI_raw'].max()\n",
        "\n",
        "    # Get the normalization target from the configuration.\n",
        "    norm_target = config['finalization_parameters']['normalization_target_max']\n",
        "\n",
        "    # Normalize the series. If the max value is 0, the result is an all-zero series.\n",
        "    # normalized_CRUI = (CRUI_raw / max(CRUI_raw)) * 100\n",
        "    if max_raw_crui > 0:\n",
        "        df_final['CRUI'] = (df_final['CRUI_raw'] / max_raw_crui) * norm_target\n",
        "    else:\n",
        "        # If no COVID-related uncertainty was ever detected, the index is zero everywhere.\n",
        "        df_final['CRUI'] = 0.0\n",
        "\n",
        "    return df_final\n"
      ],
      "metadata": {
        "id": "fyXsFveHxIbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Data Preparation for VAR Analysis\n",
        "\n",
        "def prepare_data_for_var(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Prepares the dataset for Vector Autoregression (VAR) analysis.\n",
        "\n",
        "    This function executes a rigorous, multi-step pipeline to transform the\n",
        "    raw data into a stationary format suitable for VAR modeling, strictly\n",
        "    adhering to the specified econometric methodology.\n",
        "\n",
        "    The pipeline includes:\n",
        "    1.  Integration: Selects the 10 specified VAR variables and handles any\n",
        "        missing macroeconomic data via linear interpolation.\n",
        "    2.  Stationarity Testing (Pre-Transform): Conducts and logs ADF tests on\n",
        "        the initial data to establish a baseline.\n",
        "    3.  Logarithmic Transformation: Applies natural logs to specified variables\n",
        "        to stabilize variance, with robust checks for non-positive values.\n",
        "    4.  First-Differencing: Applies first-order differencing to all variables\n",
        "        to induce stationarity.\n",
        "    5.  Stationarity Testing (Post-Transform): Conducts and logs a final round\n",
        "        of ADF tests to confirm that all variables are stationary before estimation.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The fully constructed DataFrame containing the BRUI\n",
        "            and all macroeconomic variables.\n",
        "        config (Dict[str, Any]): The `econometric_analysis_config` dictionary\n",
        "            that specifies all parameters for the VAR analysis.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]: A tuple containing:\n",
        "            - The final, stationary DataFrame ready for VAR estimation.\n",
        "            - A detailed audit log documenting every test and transformation.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If required variables or configuration keys are missing.\n",
        "        ValueError: If a variable intended for log transformation contains\n",
        "            non-positive values.\n",
        "    \"\"\"\n",
        "    # --- Initialize Audit Log ---\n",
        "    audit_log: Dict[str, Any] = {\n",
        "        \"integration\": {},\n",
        "        \"stationarity_pre_transform\": {},\n",
        "        \"transformations\": {},\n",
        "        \"stationarity_post_transform\": {}\n",
        "    }\n",
        "\n",
        "    # --- Step 1: BRUI Integration with Macroeconomic Variables ---\n",
        "    try:\n",
        "        # Select the exact list of variables required for the VAR model.\n",
        "        var_list = config['model_specification']['variables']\n",
        "        df_var = df[var_list].copy()\n",
        "    except KeyError as e:\n",
        "        raise KeyError(f\"A required variable for the VAR model is missing from the DataFrame or config: {e}\")\n",
        "\n",
        "    # Handle missing values in the macroeconomic data via interpolation.\n",
        "    # This is a standard approach for filling small gaps in monthly series.\n",
        "    initial_nan_counts = df_var.isna().sum()\n",
        "    audit_log['integration']['initial_nan_counts'] = initial_nan_counts[initial_nan_counts > 0].to_dict()\n",
        "\n",
        "    # Apply linear interpolation, then back-fill and forward-fill for edge cases.\n",
        "    df_var.interpolate(method='linear', limit_direction='both', inplace=True)\n",
        "\n",
        "    final_nan_counts = df_var.isna().sum().sum()\n",
        "    if final_nan_counts > 0:\n",
        "        # This should not happen with the above method but is a critical check.\n",
        "        raise ValueError(\"Data interpolation failed. NaN values still exist in the VAR dataset.\")\n",
        "\n",
        "    audit_log['integration']['final_row_count'] = len(df_var)\n",
        "\n",
        "    # --- Step 2: Comprehensive Stationarity Testing (Pre-Transformation) ---\n",
        "    def _run_adf_test(data: pd.DataFrame) -> Dict[str, Any]:\n",
        "        \"\"\"Helper to run and format ADF tests on all columns of a DataFrame.\"\"\"\n",
        "        results = {}\n",
        "        for name, series in data.items():\n",
        "            # Perform the ADF test with automatic lag selection based on AIC.\n",
        "            adf_result = adfuller(series.dropna(), autolag='AIC')\n",
        "            # Format the results into a clean dictionary.\n",
        "            results[name] = {\n",
        "                'test_statistic': adf_result[0],\n",
        "                'p_value': adf_result[1],\n",
        "                'lags_used': adf_result[2],\n",
        "                'critical_values': adf_result[4],\n",
        "                # A series is stationary if the p-value is below the 0.05 threshold.\n",
        "                'is_stationary_at_5%': adf_result[1] < 0.05\n",
        "            }\n",
        "        return results\n",
        "\n",
        "    audit_log['stationarity_pre_transform'] = _run_adf_test(df_var)\n",
        "\n",
        "    # --- Step 3: Logarithmic Transformation ---\n",
        "    try:\n",
        "        log_vars = config['data_transformation_parameters']['log_transform_variables']\n",
        "    except KeyError as e:\n",
        "        raise KeyError(f\"Missing 'log_transform_variables' key in config: {e}\")\n",
        "\n",
        "    # Pre-flight check: Ensure all values in columns to be logged are positive.\n",
        "    for col in log_vars:\n",
        "        if (df_var[col] <= 0).any():\n",
        "            raise ValueError(\n",
        "                f\"Column '{col}' contains non-positive values and cannot be \"\n",
        "                \"log-transformed. Please clean the data.\"\n",
        "            )\n",
        "\n",
        "    # Apply the natural logarithm transformation.\n",
        "    df_var[log_vars] = np.log(df_var[log_vars])\n",
        "    audit_log['transformations']['log_transformed_variables'] = log_vars\n",
        "\n",
        "    # --- Step 4: First-Differencing ---\n",
        "    try:\n",
        "        diff_order = config['data_transformation_parameters']['differencing_order']\n",
        "    except KeyError as e:\n",
        "        raise KeyError(f\"Missing 'differencing_order' key in config: {e}\")\n",
        "\n",
        "    # Apply first-order differencing to all columns to induce stationarity.\n",
        "    df_stationary = df_var.diff(periods=diff_order)\n",
        "\n",
        "    # Differencing creates NaNs in the first row(s); these must be removed.\n",
        "    df_stationary.dropna(inplace=True)\n",
        "\n",
        "    audit_log['transformations']['differencing_order'] = diff_order\n",
        "    audit_log['transformations']['final_sample_size'] = len(df_stationary)\n",
        "    audit_log['transformations']['final_date_range'] = {\n",
        "        \"start\": df_stationary.index.min().strftime('%Y-%m-%d'),\n",
        "        \"end\": df_stationary.index.max().strftime('%Y-%m-%d')\n",
        "    }\n",
        "\n",
        "    # --- Step 5: Stationarity Testing (Post-Transformation) ---\n",
        "    # This final check confirms that the data is ready for VAR estimation.\n",
        "    audit_log['stationarity_post_transform'] = _run_adf_test(df_stationary)\n",
        "\n",
        "    # Final validation: ensure all series are now stationary.\n",
        "    for var, result in audit_log['stationarity_post_transform'].items():\n",
        "        if not result['is_stationary_at_5%']:\n",
        "            # This is a critical failure of the preparation process.\n",
        "            raise ValueError(\n",
        "                f\"Stationarity not achieved for variable '{var}' after \"\n",
        "                f\"transformation (p-value: {result['p_value']:.4f}). \"\n",
        "                \"Review data or consider alternative transformations.\"\n",
        "            )\n",
        "\n",
        "    return df_stationary, audit_log\n"
      ],
      "metadata": {
        "id": "EKue6iSxySvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Vector Autoregression (VAR) Analysis\n",
        "\n",
        "def estimate_var_model(\n",
        "    df_stationary: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    max_lags: int = 12,\n",
        "    lag_selection_criterion: str = 'bic'\n",
        ") -> Tuple[VARResults, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Selects optimal lag, estimates the VAR model, and prepares for identification.\n",
        "\n",
        "    This function executes the core econometric modeling pipeline (Task 9) by:\n",
        "    1.  Determining the optimal lag order 'p' for the VAR model using the\n",
        "        specified information criteria (AIC, BIC, HQIC).\n",
        "    2.  Estimating the VAR(p) model using the correctly ordered stationary data,\n",
        "        ensuring the 'BRUI' variable is first as required for Cholesky identification.\n",
        "    3.  Performing and logging critical post-estimation diagnostic checks for\n",
        "        residual autocorrelation, normality, and model stability.\n",
        "    4.  Computing the Cholesky decomposition of the residual covariance matrix,\n",
        "        which provides the identification matrix for structural analysis.\n",
        "\n",
        "    Args:\n",
        "        df_stationary (pd.DataFrame): The stationary DataFrame from Task 8,\n",
        "            ready for VAR estimation.\n",
        "        config (Dict[str, Any]): The `econometric_analysis_config` dictionary.\n",
        "        max_lags (int): The maximum number of lags to test for lag selection.\n",
        "            Defaults to 12, appropriate for monthly data.\n",
        "        lag_selection_criterion (str): The primary criterion to use for selecting\n",
        "            the optimal lag ('aic', 'bic', or 'hqic'). Defaults to 'bic' for\n",
        "            its tendency towards parsimony.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[VARResults, Dict[str, Any]]: A tuple containing:\n",
        "            - The fitted `statsmodels.tsa.vector_ar.var_model.VARResults` object.\n",
        "            - A detailed log dictionary containing lag selection results,\n",
        "              diagnostic test outcomes, and the Cholesky identification matrix.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the VAR model is found to be unstable or if the Cholesky\n",
        "            decomposition fails due to a non-positive definite covariance matrix.\n",
        "        KeyError: If required configuration keys are missing.\n",
        "    \"\"\"\n",
        "    # --- Initialize Audit Log ---\n",
        "    estimation_log: Dict[str, Any] = {\n",
        "        \"lag_selection\": {},\n",
        "        \"estimation_summary\": {},\n",
        "        \"diagnostics\": {},\n",
        "        \"identification\": {}\n",
        "    }\n",
        "\n",
        "    # --- Step 0: Ensure Correct Variable Ordering ---\n",
        "    # The Cholesky decomposition's validity depends on the variable order\n",
        "    # during estimation. We must enforce the order specified in the config.\n",
        "    try:\n",
        "        cholesky_order = config['impulse_response_parameters']['cholesky_variable_order']\n",
        "        df_ordered = df_stationary[cholesky_order].copy()\n",
        "    except KeyError as e:\n",
        "        raise KeyError(f\"A required variable for Cholesky ordering is missing or config is invalid: {e}\")\n",
        "\n",
        "    # --- Step 1: Optimal Lag Length Selection ---\n",
        "    # Instantiate a VAR model on the ordered data to select the lag order.\n",
        "    model_for_lag_selection = VAR(df_ordered)\n",
        "\n",
        "    # Use the select_order method to test lags up to max_lags.\n",
        "    lag_selection_results = model_for_lag_selection.select_order(maxlags=max_lags)\n",
        "\n",
        "    # Store the full results table in the log for auditability.\n",
        "    estimation_log['lag_selection']['full_results'] = lag_selection_results.summary().as_html()\n",
        "\n",
        "    # Select the optimal lag based on the chosen criterion.\n",
        "    optimal_lag = lag_selection_results.selected_lags[lag_selection_criterion]\n",
        "    estimation_log['lag_selection']['selected_criterion'] = lag_selection_criterion\n",
        "    estimation_log['lag_selection']['optimal_lag_p'] = optimal_lag\n",
        "\n",
        "    # --- Step 2: VAR Model Estimation ---\n",
        "    # Instantiate the final VAR model with the correctly ordered data.\n",
        "    model = VAR(df_ordered)\n",
        "\n",
        "    # Fit the model using the optimal lag order determined in the previous step.\n",
        "    # Y_t = c + A_1*Y_{t-1} + ... + A_p*Y_{t-p} + u_t\n",
        "    try:\n",
        "        results = model.fit(optimal_lag)\n",
        "    except np.linalg.LinAlgError as e:\n",
        "        raise RuntimeError(f\"VAR model estimation failed due to a linear algebra error: {e}\")\n",
        "\n",
        "    # Log the summary of the estimated model.\n",
        "    estimation_log['estimation_summary']['model_summary'] = results.summary().as_html()\n",
        "\n",
        "    # --- Post-Estimation Diagnostics ---\n",
        "    # a) Test for residual autocorrelation (Ljung-Box test).\n",
        "    # H0: No serial correlation. We want high p-values.\n",
        "    corr_test = results.test_serial_correlation(lags=optimal_lag + 5)\n",
        "    estimation_log['diagnostics']['serial_correlation_p_value'] = corr_test.pvalue\n",
        "\n",
        "    # b) Test for residual normality (Jarque-Bera test).\n",
        "    # H0: Residuals are normally distributed.\n",
        "    norm_test = results.test_normality()\n",
        "    estimation_log['diagnostics']['normality_p_value'] = norm_test.pvalue\n",
        "\n",
        "    # c) Test for model stability.\n",
        "    # The model is stable if all roots of the companion matrix have modulus < 1.\n",
        "    is_stable = results.is_stable()\n",
        "    estimation_log['diagnostics']['is_stable'] = is_stable\n",
        "    if not is_stable:\n",
        "        raise ValueError(\"Estimated VAR model is unstable. Cannot proceed with analysis.\")\n",
        "\n",
        "    # --- Step 3: Cholesky Decomposition for Structural Identification ---\n",
        "    # This is the key to identifying structural shocks from reduced-form residuals.\n",
        "    # u_t = P * epsilon_t, where P is the lower-triangular Cholesky factor.\n",
        "\n",
        "    # Get the residual covariance matrix (Sigma_u) from the results.\n",
        "    sigma_u = results.sigma_u\n",
        "\n",
        "    try:\n",
        "        # Compute the lower-triangular Cholesky factor P.\n",
        "        identification_matrix_p = np.linalg.cholesky(sigma_u)\n",
        "    except np.linalg.LinAlgError:\n",
        "        raise ValueError(\n",
        "            \"Cholesky decomposition failed. The residual covariance matrix \"\n",
        "            \"is not positive definite, indicating a severe model issue.\"\n",
        "        )\n",
        "\n",
        "    # Store the identification matrix in the log, with clear labels.\n",
        "    estimation_log['identification']['cholesky_matrix_P'] = pd.DataFrame(\n",
        "        identification_matrix_p,\n",
        "        index=df_ordered.columns,\n",
        "        columns=df_ordered.columns\n",
        "    )\n",
        "    estimation_log['identification']['strategy'] = \"Cholesky Decomposition\"\n",
        "    estimation_log['identification']['ordering_assumption'] = cholesky_order\n",
        "\n",
        "    return results, estimation_log\n"
      ],
      "metadata": {
        "id": "SDzcfJlCzaLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Post-Estimation Analysis\n",
        "\n",
        "def run_post_estimation_analysis(\n",
        "    var_results: VARResults,\n",
        "    estimation_log: Dict[str, Any],\n",
        "    config: Dict[str, Any],\n",
        "    random_seed: int = 42\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Conducts post-estimation analysis on a fitted VAR model.\n",
        "\n",
        "    This function performs the three key post-estimation analyses required by\n",
        "    the research methodology:\n",
        "    1.  Impulse Response Functions (IRFs): Calculates the dynamic response of\n",
        "        each variable to an identified one-standard-deviation structural shock\n",
        "        in every other variable, using the pre-computed Cholesky identification.\n",
        "    2.  Forecast Error Variance Decompositions (FEVDs): Quantifies the\n",
        "        proportion of movement in each variable that is attributable to shocks\n",
        "        from the other variables over a given horizon.\n",
        "    3.  Bootstrapped Confidence Intervals: Generates 90% confidence intervals\n",
        "        for the IRFs using the specified percentile bootstrap method with 999\n",
        "        repetitions to provide a measure of statistical uncertainty.\n",
        "\n",
        "    Args:\n",
        "        var_results (VARResults): The fitted `statsmodels` VAR results object\n",
        "            from Task 9.\n",
        "        estimation_log (Dict[str, Any]): The log from Task 9, containing the\n",
        "            Cholesky identification matrix.\n",
        "        config (Dict[str, Any]): The `econometric_analysis_config` dictionary.\n",
        "        random_seed (int): A seed for the random number generator to ensure\n",
        "            reproducibility of the bootstrap. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the structured results for\n",
        "            'irf', 'fevd', and 'irf_confidence_intervals'.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If required configuration keys or the Cholesky matrix\n",
        "            are missing.\n",
        "    \"\"\"\n",
        "    # --- Setup and Parameter Extraction ---\n",
        "    try:\n",
        "        # Extract parameters from the configuration dictionary.\n",
        "        horizon = config['impulse_response_parameters']['horizon']\n",
        "        ci_level = config['impulse_response_parameters']['confidence_interval_level']\n",
        "        bootstrap_reps = config['impulse_response_parameters']['bootstrap_repetitions']\n",
        "\n",
        "        # Extract the pre-computed Cholesky identification matrix.\n",
        "        cholesky_matrix = estimation_log['identification']['cholesky_matrix_P'].values\n",
        "    except KeyError as e:\n",
        "        raise KeyError(f\"A required configuration or log key is missing: {e}\")\n",
        "\n",
        "    # The variable names in the correct, estimated order.\n",
        "    var_names = var_results.names\n",
        "\n",
        "    # --- Step 1: Impulse Response Function (IRF) Calculation ---\n",
        "    # Instantiate the IRF analysis object from the VAR results.\n",
        "    # The formula IRF(h) = Phi_h * P is computed internally.\n",
        "    irf_analysis = var_results.irf(periods=horizon, var_decomp=cholesky_matrix)\n",
        "\n",
        "    # Extract the point estimates of the orthogonalized IRFs.\n",
        "    # Shape: (horizon+1, k, k) -> response of col to impulse in row.\n",
        "    irf_point_estimates = irf_analysis.orth_irfs\n",
        "\n",
        "    # --- Step 2: Forecast Error Variance Decomposition (FEVD) ---\n",
        "    # Compute the FEVDs using the fitted model.\n",
        "    fevd_results = var_results.fevd(periods=horizon + 1, var_decomp=cholesky_matrix)\n",
        "\n",
        "    # The result is a VARResults object; we extract the summary DataFrame.\n",
        "    # The FEVD shows the percentage of the forecast error variance of the variable\n",
        "    # in the column that is explained by innovations to the variable in the row.\n",
        "    fevd_summary = fevd_results.summary()\n",
        "\n",
        "    # --- Step 3: Percentile Bootstrap Confidence Interval Estimation ---\n",
        "    # This is a computationally intensive step.\n",
        "    # Set the random seed for perfect reproducibility of the bootstrap.\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # Calculate the confidence intervals using the built-in bootstrap method.\n",
        "    # We specify the number of repetitions and the desired significance level.\n",
        "    # The significance level alpha is 1 - confidence_level.\n",
        "    alpha = 1 - ci_level\n",
        "    lower_quantile = alpha / 2\n",
        "    upper_quantile = 1 - (alpha / 2)\n",
        "\n",
        "    # The `err_band_kws` argument allows us to specify bootstrap parameters.\n",
        "    # This computes the lower and upper bands for the confidence interval.\n",
        "    irf_lower_bounds, irf_upper_bounds = irf_analysis.err_band(\n",
        "        orth=True,\n",
        "        repl=bootstrap_reps,\n",
        "        signif=alpha,\n",
        "        seed=random_seed,\n",
        "        burn=0, # No burn-in needed for this type of bootstrap\n",
        "        component=None # For all components\n",
        "    )\n",
        "\n",
        "    # --- Structure and Return Results ---\n",
        "    # Organize the results into a clean, well-structured dictionary.\n",
        "    post_estimation_results = {\n",
        "        'irf': {\n",
        "            'point_estimates': irf_point_estimates,\n",
        "            'description': \"3D array (horizon, responding_var, impulse_var) of IRF point estimates.\"\n",
        "        },\n",
        "        'irf_confidence_intervals': {\n",
        "            'lower_bound': irf_lower_bounds,\n",
        "            'upper_bound': irf_upper_bounds,\n",
        "            'confidence_level': ci_level,\n",
        "            'bootstrap_repetitions': bootstrap_reps,\n",
        "            'description': \"3D arrays for lower and upper CI bounds, matching the IRF point estimates.\"\n",
        "        },\n",
        "        'fevd': {\n",
        "            'summary_tables': fevd_summary,\n",
        "            'description': \"List of DataFrames, one for each variable's FEVD.\"\n",
        "        },\n",
        "        'metadata': {\n",
        "            'horizon': horizon,\n",
        "            'variable_names': var_names,\n",
        "            'random_seed_used': random_seed\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return post_estimation_results\n"
      ],
      "metadata": {
        "id": "DE7i7-ty0is9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Visualization\n",
        "\n",
        "def plot_brui_with_events(\n",
        "    brui_series: pd.Series,\n",
        "    events: Dict[str, str]\n",
        ") -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Generates a publication-quality time series plot of the BRUI with annotations.\n",
        "\n",
        "    This function visualizes the final, normalized BRUI over its entire sample\n",
        "    period. It is enhanced with a collision-aware algorithm to intelligently\n",
        "    place annotations for key historical events, ensuring readability even when\n",
        "    events are chronologically close. This replicates the style and content of\n",
        "    Figure 2 from the research paper with superior robustness.\n",
        "\n",
        "    Args:\n",
        "        brui_series (pd.Series): A pandas Series containing the final BRUI values,\n",
        "            with a DatetimeIndex.\n",
        "        events (Dict[str, str]): A dictionary where keys are date strings\n",
        "            (e.g., 'YYYY-MM-DD') and values are the event descriptions.\n",
        "\n",
        "    Returns:\n",
        "        plt.Figure: The matplotlib Figure object containing the plot.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input Series does not have a DatetimeIndex.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check if the index of the input Series is a DatetimeIndex.\n",
        "    if not isinstance(brui_series.index, pd.DatetimeIndex):\n",
        "        # Raise an error if the index is not of the correct type.\n",
        "        raise ValueError(\"Input 'brui_series' must have a DatetimeIndex.\")\n",
        "\n",
        "    # --- Plot Styling Setup ---\n",
        "    # Set a professional and clean plot style.\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    # Create a Figure and a single Axes object with a specified size and resolution.\n",
        "    fig, ax = plt.subplots(figsize=(16, 9), dpi=200)\n",
        "\n",
        "    # --- Plotting the Main Time Series ---\n",
        "    # Plot the BRUI data with a professional color and line width.\n",
        "    ax.plot(brui_series.index, brui_series, color='#003366', lw=2.5, label='BRUI')\n",
        "\n",
        "    # --- Remediation: Collision-Aware Annotation Placement ---\n",
        "    # Define a list of preferred vertical offsets for annotations (in data coordinates).\n",
        "    y_offsets = [brui_series.max() * offset for offset in [0.1, 0.25, 0.4, 0.18, 0.33]]\n",
        "    # Initialize a list to store the bounding boxes of placed annotations.\n",
        "    placed_annotations_bboxes = []\n",
        "\n",
        "    # Sort events by date to process them chronologically.\n",
        "    sorted_events = sorted(events.items(), key=lambda item: pd.to_datetime(item[0]))\n",
        "\n",
        "    # Iterate through each event to be annotated on the plot.\n",
        "    for date_str, description in sorted_events:\n",
        "        # Convert the event's date string to a pandas Timestamp object.\n",
        "        event_date = pd.to_datetime(date_str)\n",
        "\n",
        "        # Proceed only if the event date falls within the plotted data's range.\n",
        "        if brui_series.index.min() <= event_date <= brui_series.index.max():\n",
        "            # Draw a vertical line on the plot to mark the event's date.\n",
        "            ax.axvline(x=event_date, color='red', linestyle='--', linewidth=1.0, alpha=0.75)\n",
        "\n",
        "            # Find the y-position on the BRUI line corresponding to the event date.\n",
        "            y_pos = brui_series.asof(event_date)\n",
        "\n",
        "            # Initialize the best position for the annotation text.\n",
        "            best_y = y_pos + y_offsets[0]\n",
        "\n",
        "            # Create a dummy annotation to find the best non-colliding position.\n",
        "            # This annotation is initially invisible.\n",
        "            annotation = ax.annotate(\n",
        "                description,\n",
        "                xy=(event_date, y_pos),\n",
        "                xytext=(event_date, best_y), # Start with the first preferred offset.\n",
        "                arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=4),\n",
        "                ha='center',\n",
        "                fontsize=9,\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", lw=0.5, alpha=0.95),\n",
        "                visible=False # Keep it invisible until the best position is found.\n",
        "            )\n",
        "\n",
        "            # Force matplotlib to draw the canvas to calculate object dimensions.\n",
        "            fig.canvas.draw()\n",
        "\n",
        "            # --- Iterative Placement Logic ---\n",
        "            # Assume the initial position is not ideal until proven otherwise.\n",
        "            is_placed = False\n",
        "            # Iterate through the preferred vertical offsets.\n",
        "            for offset in y_offsets:\n",
        "                # Set the hypothetical y-position for the annotation text.\n",
        "                annotation.set_position((event_date, y_pos + offset))\n",
        "                # Get the bounding box of the annotation in display coordinates.\n",
        "                hypothetical_bbox = annotation.get_bbox_patch().get_window_extent(fig.canvas.renderer)\n",
        "\n",
        "                # Check if this hypothetical box collides with any already placed boxes.\n",
        "                is_colliding = any(\n",
        "                    hypothetical_bbox.overlaps(placed_bbox) for placed_bbox in placed_annotations_bboxes\n",
        "                )\n",
        "\n",
        "                # If there is no collision, this is a good position.\n",
        "                if not is_colliding:\n",
        "                    # Make the annotation visible at this position.\n",
        "                    annotation.set_visible(True)\n",
        "                    # Add its bounding box to the list of placed annotations.\n",
        "                    placed_annotations_bboxes.append(hypothetical_bbox)\n",
        "                    # Mark as placed and break the inner loop.\n",
        "                    is_placed = True\n",
        "                    break\n",
        "\n",
        "            # If all preferred positions resulted in a collision, place it at the default.\n",
        "            if not is_placed:\n",
        "                # Set the position to the first (closest) offset as a fallback.\n",
        "                annotation.set_position((event_date, y_pos + y_offsets[0]))\n",
        "                # Make the annotation visible.\n",
        "                annotation.set_visible(True)\n",
        "\n",
        "    # --- Final Plot Formatting ---\n",
        "    # Set the main title of the plot with appropriate styling.\n",
        "    ax.set_title('Brexit-Related Uncertainty Index (BRUI) and Major Events', fontsize=18, weight='bold', pad=20)\n",
        "    # Set the label for the y-axis.\n",
        "    ax.set_ylabel('BRUI (Normalized, Max=100)', fontsize=14)\n",
        "    # Set the label for the x-axis.\n",
        "    ax.set_xlabel('Year', fontsize=14)\n",
        "    # Set the y-axis limits to provide padding for annotations.\n",
        "    ax.set_ylim(0, brui_series.max() * 1.5)\n",
        "    # Display the plot legend.\n",
        "    ax.legend(loc='upper left', fontsize=12)\n",
        "\n",
        "    # Configure the x-axis to show years clearly.\n",
        "    ax.xaxis.set_major_locator(mdates.YearLocator(base=2))\n",
        "    # Set the format of the year display on the x-axis.\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "    # Rotate the x-axis tick labels for better readability.\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    # Adjust the plot layout to prevent labels from being cut off.\n",
        "    fig.tight_layout()\n",
        "\n",
        "    # Return the final Figure object.\n",
        "    return fig\n",
        "\n",
        "def plot_comparative_validation(\n",
        "    df_indices: pd.DataFrame,\n",
        "    brui_col: str,\n",
        "    comparison_cols: List[str]\n",
        ") -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Generates a multi-panel plot comparing the BRUI to other indices.\n",
        "\n",
        "    This function replicates Figures 3, 4, and 5 from the paper, creating\n",
        "    subplots to visually compare the calculated BRUI against other established\n",
        "    Brexit uncertainty indices. It calculates and displays the Pearson\n",
        "    correlation coefficient for the overlapping period in each subplot.\n",
        "\n",
        "    Args:\n",
        "        df_indices (pd.DataFrame): A DataFrame containing the BRUI and all\n",
        "            comparison indices, with a DatetimeIndex.\n",
        "        brui_col (str): The name of the column containing the calculated BRUI.\n",
        "        comparison_cols (List[str]): A list of column names for the indices\n",
        "            to be compared against the BRUI.\n",
        "\n",
        "    Returns:\n",
        "        plt.Figure: The matplotlib Figure object containing the grid of plots.\n",
        "    \"\"\"\n",
        "    # --- Plot Styling and Layout Setup ---\n",
        "    # Set a professional and clean plot style.\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    # Get the number of comparisons to determine the number of subplots.\n",
        "    n_comparisons = len(comparison_cols)\n",
        "    # Create a figure and a grid of subplots.\n",
        "    fig, axes = plt.subplots(\n",
        "        nrows=n_comparisons, ncols=1, figsize=(12, 6 * n_comparisons),\n",
        "        sharex=True, dpi=150\n",
        "    )\n",
        "    # Ensure 'axes' is always a list-like object for consistent indexing, even with one subplot.\n",
        "    if n_comparisons == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    # --- Loop Through Each Comparison ---\n",
        "    # Iterate through the axes and the names of the comparison columns simultaneously.\n",
        "    for ax, comp_col in zip(axes, comparison_cols):\n",
        "        # Create a temporary DataFrame with only the two series to be compared.\n",
        "        df_comp = df_indices[[brui_col, comp_col]].copy()\n",
        "\n",
        "        # Drop rows with any NaN values to find the overlapping time period.\n",
        "        df_overlap = df_comp.dropna()\n",
        "\n",
        "        # Check if there is enough overlapping data to calculate a correlation.\n",
        "        if len(df_overlap) > 1:\n",
        "            # Calculate the Pearson correlation coefficient and the p-value.\n",
        "            corr, p_value = pearsonr(df_overlap[brui_col], df_overlap[comp_col])\n",
        "            # Format the correlation text to be displayed on the plot.\n",
        "            corr_text = f'Correlation: {corr:.2f} (p-value: {p_value:.3f})'\n",
        "        else:\n",
        "            # Set a default text if there is no overlapping data.\n",
        "            corr_text = 'Correlation: N/A (No Overlap)'\n",
        "\n",
        "        # Plot the BRUI series on the current subplot.\n",
        "        ax.plot(df_comp.index, df_comp[brui_col], label=brui_col, color='#003366', lw=2)\n",
        "        # Plot the comparison index series on the same subplot with a different style.\n",
        "        ax.plot(df_comp.index, df_comp[comp_col], label=comp_col, color='#D55E00', lw=2, linestyle='--')\n",
        "\n",
        "        # --- Formatting for Each Subplot ---\n",
        "        # Set the title for the subplot, including the calculated correlation.\n",
        "        ax.set_title(f'Comparison of {brui_col} with {comp_col}\\n({corr_text})', fontsize=14, weight='bold')\n",
        "        # Set the y-axis label for the subplot.\n",
        "        ax.set_ylabel('Index Value', fontsize=10)\n",
        "        # Display the legend for the subplot.\n",
        "        ax.legend(loc='upper left')\n",
        "        # Add a grid for better readability.\n",
        "        ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "    # --- Final Global Formatting ---\n",
        "    # Set the x-axis label only on the bottom-most subplot.\n",
        "    axes[-1].set_xlabel('Year', fontsize=12)\n",
        "    # Set a main title for the entire figure.\n",
        "    fig.suptitle('BRUI Validation Against Alternative Indices', fontsize=18, weight='bold', y=1.02)\n",
        "    # Adjust the plot layout to prevent titles/labels from overlapping.\n",
        "    fig.tight_layout(rect=[0, 0, 1, 1])\n",
        "\n",
        "    # Return the final Figure object.\n",
        "    return fig\n",
        "\n",
        "def plot_impulse_response_functions(\n",
        "    irf_results: Dict[str, Any],\n",
        "    shock_variable: str = 'BRUI'\n",
        ") -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Generates a grid plot of Impulse Response Functions with confidence bands.\n",
        "\n",
        "    This function visualizes the dynamic responses of all macroeconomic variables\n",
        "    to a one-standard-deviation shock in the specified shock variable (typically\n",
        "    BRUI), replicating the style of Figure 6 from the paper.\n",
        "\n",
        "    Args:\n",
        "        irf_results (Dict[str, Any]): The post-estimation results dictionary\n",
        "            from Task 10, containing IRF point estimates and confidence intervals.\n",
        "        shock_variable (str): The name of the variable whose shock is being analyzed.\n",
        "\n",
        "    Returns:\n",
        "        plt.Figure: The matplotlib Figure object containing the grid of IRF plots.\n",
        "    \"\"\"\n",
        "    # --- Extract Data from Results Dictionary ---\n",
        "    # Get the 3D array of IRF point estimates.\n",
        "    point_estimates = irf_results['irf']['point_estimates']\n",
        "    # Get the 3D array for the lower bound of the confidence interval.\n",
        "    lower_bounds = irf_results['irf_confidence_intervals']['lower_bound']\n",
        "    # Get the 3D array for the upper bound of the confidence interval.\n",
        "    upper_bounds = irf_results['irf_confidence_intervals']['upper_bound']\n",
        "    # Get the list of variable names in the correct order.\n",
        "    var_names = irf_results['metadata']['variable_names']\n",
        "    # Get the number of periods for the IRF horizon.\n",
        "    horizon = irf_results['metadata']['horizon']\n",
        "\n",
        "    # Find the integer index of the variable that is originating the shock.\n",
        "    try:\n",
        "        shock_idx = var_names.index(shock_variable)\n",
        "    except ValueError:\n",
        "        # Raise an error if the specified shock variable is not in the model.\n",
        "        raise ValueError(f\"Shock variable '{shock_variable}' not found in the model variables.\")\n",
        "\n",
        "    # --- Plot Styling and Layout Setup ---\n",
        "    # Set a professional and clean plot style.\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    # Identify the variables that will be responding to the shock.\n",
        "    response_vars = [v for v in var_names] # Plot all responses, including to itself.\n",
        "    # Get the total number of response variables.\n",
        "    n_responses = len(response_vars)\n",
        "    # Define the grid layout for the subplots (e.g., 4x3 for 10 variables).\n",
        "    n_cols = 3\n",
        "    # Calculate the required number of rows for the grid.\n",
        "    n_rows = int(np.ceil(n_responses / n_cols))\n",
        "\n",
        "    # Create a figure and a grid of subplots.\n",
        "    fig, axes = plt.subplots(\n",
        "        nrows=n_rows, ncols=n_cols, figsize=(15, 4 * n_rows),\n",
        "        sharex=True, dpi=200\n",
        "    )\n",
        "    # Flatten the 2D array of axes into a 1D array for easy iteration.\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # --- Loop Through Each Response Variable ---\n",
        "    # Iterate through the flattened axes and the list of response variables.\n",
        "    for i, response_var in enumerate(response_vars):\n",
        "        # Select the current axis object for plotting.\n",
        "        ax = axes[i]\n",
        "        # Get the integer index of the current response variable.\n",
        "        response_idx = var_names.index(response_var)\n",
        "\n",
        "        # Extract the specific 1D IRF series for this shock-response pair.\n",
        "        irf_series = point_estimates[:, response_idx, shock_idx]\n",
        "        # Extract the corresponding lower confidence bound series.\n",
        "        lower_series = lower_bounds[:, response_idx, shock_idx]\n",
        "        # Extract the corresponding upper confidence bound series.\n",
        "        upper_series = upper_bounds[:, response_idx, shock_idx]\n",
        "\n",
        "        # Define the x-axis as the horizon in months (from 0 to H).\n",
        "        x_axis = range(horizon + 1)\n",
        "\n",
        "        # Plot the point estimate of the impulse response.\n",
        "        ax.plot(x_axis, irf_series, color='#003366', lw=2.5, label='Point Estimate')\n",
        "\n",
        "        # Shade the area between the lower and upper bounds to show the confidence interval.\n",
        "        ax.fill_between(x_axis, lower_series, upper_series, color='#0072B2', alpha=0.2, label='90% CI')\n",
        "\n",
        "        # Plot a horizontal line at zero, which is the critical reference for significance.\n",
        "        ax.axhline(0, color='black', linestyle='--', linewidth=1)\n",
        "\n",
        "        # --- Formatting for Each Subplot ---\n",
        "        # Set the title for the subplot, indicating the response variable.\n",
        "        ax.set_title(f'Response of {response_var}', fontsize=12, weight='bold')\n",
        "        # Set the x-axis limits to the specified horizon.\n",
        "        ax.set_xlim(0, horizon)\n",
        "        # Add a grid for better readability.\n",
        "        ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "        # Add a y-axis label only to the plots in the first column.\n",
        "        if i % n_cols == 0:\n",
        "            ax.set_ylabel('Response Magnitude', fontsize=10)\n",
        "        # Add an x-axis label only to the plots in the bottom row.\n",
        "        if i >= (n_rows - 1) * n_cols:\n",
        "            ax.set_xlabel('Months After Shock', fontsize=10)\n",
        "\n",
        "    # --- Final Global Formatting ---\n",
        "    # Hide any unused subplots if the number of responses is not a multiple of n_cols.\n",
        "    for i in range(n_responses, len(axes)):\n",
        "        # Make the unused axis invisible.\n",
        "        axes[i].set_visible(False)\n",
        "\n",
        "    # Set a main title for the entire figure.\n",
        "    fig.suptitle(f'Impulse Responses to a One S.D. Shock in {shock_variable}', fontsize=18, weight='bold')\n",
        "    # Adjust the plot layout to make space for the main title.\n",
        "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "    # Return the final Figure object.\n",
        "    return fig\n",
        "\n"
      ],
      "metadata": {
        "id": "Dz5PzsIV2Ybg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline\n",
        "\n",
        "def run_brexit_uncertainty_analysis(\n",
        "    df_input: pd.DataFrame,\n",
        "    uncertainty_lexicon: List[str],\n",
        "    brexit_lexicon: List[str],\n",
        "    covid_lexicon: List[str],\n",
        "    index_construction_config: Dict[str, Any],\n",
        "    econometric_analysis_config: Dict[str, Any],\n",
        "    brexit_events_for_plotting: Dict[str, str],\n",
        "    comparison_indices_df: pd.DataFrame = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the end-to-end research pipeline for the Brexit Uncertainty Index.\n",
        "\n",
        "    This orchestrator function serves as the master controller for the entire\n",
        "    analysis. It sequentially executes all tasks from parameter validation to\n",
        "    final visualization, ensuring a rigorous, reproducible, and auditable\n",
        "    workflow. Each step's outputs are programmatically passed to the next, and\n",
        "    all significant results, data, and logs are compiled into a comprehensive\n",
        "    final dictionary. This version is updated to align with the fully remediated\n",
        "    and professional-grade sub-components.\n",
        "\n",
        "    Args:\n",
        "        df_input (pd.DataFrame): The raw input DataFrame containing monthly\n",
        "            macroeconomic data and the EIU text corpus.\n",
        "        uncertainty_lexicon (List[str]): The raw list of uncertainty keywords.\n",
        "        brexit_lexicon (List[str]): The raw list of Brexit-related keywords.\n",
        "        covid_lexicon (List[str]): The raw list of COVID-19 related keywords.\n",
        "        index_construction_config (Dict[str, Any]): Configuration for the\n",
        "            text-based index construction.\n",
        "        econometric_analysis_config (Dict[str, Any]): Configuration for the\n",
        "            econometric VAR analysis.\n",
        "        brexit_events_for_plotting (Dict[str, str]): A dictionary of key Brexit\n",
        "            events for annotating the final BRUI time-series plot.\n",
        "        comparison_indices_df (pd.DataFrame, optional): A DataFrame containing\n",
        "            alternative indices (e.g., BRUI_B, BRUI_C) for validation plotting.\n",
        "            Must have a DatetimeIndex. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing all results,\n",
        "            including final data, audit logs, fitted models, post-estimation\n",
        "            analysis, and generated figures.\n",
        "    \"\"\"\n",
        "    # Initialize the master results dictionary to store all pipeline artifacts.\n",
        "    results: Dict[str, Any] = {\n",
        "        \"audit_logs\": {},\n",
        "        \"final_data\": {},\n",
        "        \"fitted_model\": {},\n",
        "        \"analysis_results\": {},\n",
        "        \"visualizations\": {}\n",
        "    }\n",
        "\n",
        "    # --- Task 0: Parameter Validation ---\n",
        "    # Provide user feedback that the process is starting.\n",
        "    print(\"Executing Task 0: Parameter Validation...\")\n",
        "    # Call the validation function as a critical quality gate before any computation.\n",
        "    validate_parameters(\n",
        "        df=df_input,\n",
        "        uncertainty_lexicon=uncertainty_lexicon,\n",
        "        brexit_lexicon=brexit_lexicon,\n",
        "        covid_lexicon=covid_lexicon,\n",
        "        index_construction_config=index_construction_config,\n",
        "        econometric_analysis_config=econometric_analysis_config\n",
        "    )\n",
        "    # Confirm successful validation to the user.\n",
        "    print(\"...Validation successful.\")\n",
        "\n",
        "    # --- Task 1: Data Cleansing ---\n",
        "    # Announce the start of the data cleansing task.\n",
        "    print(\"Executing Task 1: Data Cleansing...\")\n",
        "    # Call the remediated cleanse_data function, which performs targeted cleaning.\n",
        "    df_clean, audit_log_1 = cleanse_data(df_input, index_construction_config)\n",
        "    # Store the detailed audit log from the cleansing process.\n",
        "    results[\"audit_logs\"][\"data_cleansing\"] = audit_log_1\n",
        "    # Confirm task completion.\n",
        "    print(\"...Data cleansing complete.\")\n",
        "\n",
        "    # --- Task 2: Keyword List Preparation ---\n",
        "    # Announce the start of the lexicon preparation task.\n",
        "    print(\"Executing Task 2: Keyword List Preparation...\")\n",
        "    # Call the function to process raw keyword lists into an optimized structure.\n",
        "    prepared_lexicons = prepare_lexicons(uncertainty_lexicon, brexit_lexicon, covid_lexicon)\n",
        "    # Store the highly optimized lexicon object for potential review.\n",
        "    results[\"final_data\"][\"prepared_lexicons\"] = prepared_lexicons\n",
        "    # Confirm task completion.\n",
        "    print(\"...Lexicon preparation complete.\")\n",
        "\n",
        "    # --- Task 3: Text Preprocessing ---\n",
        "    # Announce the start of the NLP preprocessing pipeline.\n",
        "    print(\"Executing Task 3: Text Preprocessing...\")\n",
        "    # Amendment: Extract the language from the config to pass to the remediated function.\n",
        "    language = index_construction_config['preprocessing_parameters']['stopword_language']\n",
        "    # Call the remediated preprocessing function with the explicit language parameter.\n",
        "    df_preprocessed = preprocess_text_corpus(df_clean, prepared_lexicons, language=language)\n",
        "    # Confirm task completion.\n",
        "    print(\"...Text preprocessing complete.\")\n",
        "\n",
        "    # --- Task 4: Named Entity Recognition ---\n",
        "    # Announce the start of the NER task.\n",
        "    print(\"Executing Task 4: Named Entity Recognition...\")\n",
        "    # Load the specified SpaCy model once to avoid redundant loading.\n",
        "    nlp_model = load_spacy_model_for_ner(\n",
        "        model_name=index_construction_config['llm_parameters']['model_identifier']\n",
        "    )\n",
        "    # Apply the loaded model to the text corpus using efficient batch processing.\n",
        "    df_ner = apply_ner_to_corpus(df_preprocessed, nlp_model)\n",
        "    # Confirm task completion.\n",
        "    print(\"...NER complete.\")\n",
        "\n",
        "    # --- Task 5: Context-Aware Uncertainty Attribution ---\n",
        "    # Announce the start of the core attribution algorithm.\n",
        "    print(\"Executing Task 5: Context-Aware Uncertainty Attribution...\")\n",
        "    # Call the function to apply the context window algorithm and classify uncertainty.\n",
        "    df_attributed = attribute_uncertainty_in_corpus(df_ner, prepared_lexicons, index_construction_config)\n",
        "    # Confirm task completion.\n",
        "    print(\"...Uncertainty attribution complete.\")\n",
        "\n",
        "    # --- Task 6 & 7: Index Calculation (BRUI & CRUI) ---\n",
        "    # Announce the start of the final index calculation phase.\n",
        "    print(\"Executing Tasks 6 & 7: BRUI and CRUI Calculation...\")\n",
        "    # Calculate the BRUI based on the attributed counts.\n",
        "    df_with_brui = calculate_brui(df_attributed, index_construction_config)\n",
        "    # Calculate the CRUI using the same data for methodological consistency.\n",
        "    df_indices = calculate_crui(df_with_brui, index_construction_config)\n",
        "    # Store the final DataFrame containing both indices and all intermediate components.\n",
        "    results[\"final_data\"][\"indices_and_components\"] = df_indices\n",
        "    # Confirm task completion.\n",
        "    print(\"...Index calculation complete.\")\n",
        "\n",
        "    # --- Task 8: Data Preparation for VAR Analysis ---\n",
        "    # Announce the start of the econometric data preparation.\n",
        "    print(\"Executing Task 8: Data Preparation for VAR Analysis...\")\n",
        "    # Call the function to transform the data into a stationary format for the VAR.\n",
        "    df_stationary, audit_log_8 = prepare_data_for_var(df_indices, econometric_analysis_config)\n",
        "    # Store the detailed audit log from the transformation process.\n",
        "    results[\"audit_logs\"][\"var_data_preparation\"] = audit_log_8\n",
        "    # Store the final, stationary dataset used for estimation.\n",
        "    results[\"final_data\"][\"stationary_var_dataset\"] = df_stationary\n",
        "    # Confirm task completion.\n",
        "    print(\"...VAR data preparation complete.\")\n",
        "\n",
        "    # --- Task 9: Vector Autoregression (VAR) Analysis ---\n",
        "    # Announce the start of the VAR model estimation.\n",
        "    print(\"Executing Task 9: VAR Model Estimation...\")\n",
        "    # Call the function to select lag order, estimate the model, and run diagnostics.\n",
        "    var_results, estimation_log = estimate_var_model(df_stationary, econometric_analysis_config)\n",
        "    # Store the rich statsmodels results object.\n",
        "    results[\"fitted_model\"][\"var_results_object\"] = var_results\n",
        "    # Store the detailed log of the estimation and identification process.\n",
        "    results[\"audit_logs\"][\"var_estimation\"] = estimation_log\n",
        "    # Confirm task completion.\n",
        "    print(\"...VAR estimation complete.\")\n",
        "\n",
        "    # --- Task 10: Post-Estimation Analysis ---\n",
        "    # Announce the start of the post-estimation analysis.\n",
        "    print(\"Executing Task 10: Post-Estimation Analysis (IRF, FEVD, Bootstrap)...\")\n",
        "    # Call the function to compute IRFs, FEVDs, and bootstrapped confidence intervals.\n",
        "    post_estimation_results = run_post_estimation_analysis(var_results, estimation_log, econometric_analysis_config)\n",
        "    # Store the complete set of analytical results.\n",
        "    results[\"analysis_results\"] = post_estimation_results\n",
        "    # Confirm task completion.\n",
        "    print(\"...Post-estimation analysis complete.\")\n",
        "\n",
        "    # --- Task 11: Visualization ---\n",
        "    # Announce the start of the final visualization phase.\n",
        "    print(\"Executing Task 11: Generating Visualizations...\")\n",
        "    # Generate Figure 1: The BRUI time series annotated with key events.\n",
        "    fig1 = plot_brui_with_events(df_indices['BRUI'], brexit_events_for_plotting)\n",
        "\n",
        "    # Initialize the variable for the second figure to None.\n",
        "    fig2 = None\n",
        "    # Check if the optional DataFrame for comparison indices was provided.\n",
        "    if comparison_indices_df is not None:\n",
        "        # If so, merge the calculated BRUI with the external comparison indices.\n",
        "        df_for_comparison = df_indices[['BRUI']].join(comparison_indices_df)\n",
        "        # Generate Figure 2: The comparative validation plots.\n",
        "        fig2 = plot_comparative_validation(\n",
        "            df_for_comparison,\n",
        "            brui_col='BRUI',\n",
        "            comparison_cols=list(comparison_indices_df.columns)\n",
        "        )\n",
        "\n",
        "    # Generate Figure 3: The grid of Impulse Response Functions.\n",
        "    fig3 = plot_impulse_response_functions(\n",
        "        post_estimation_results,\n",
        "        shock_variable='BRUI'\n",
        "    )\n",
        "\n",
        "    # Store the generated matplotlib Figure objects in the results dictionary.\n",
        "    results[\"visualizations\"] = {\n",
        "        \"brui_timeline\": fig1,\n",
        "        \"comparative_validation\": fig2,\n",
        "        \"impulse_response_functions\": fig3\n",
        "    }\n",
        "    # Confirm task completion.\n",
        "    print(\"...Visualization generation complete.\")\n",
        "\n",
        "    # Announce the successful completion of the entire pipeline.\n",
        "    print(\"\\nEnd-to-end analysis pipeline executed successfully.\")\n",
        "    # Return the master dictionary containing all artifacts of the analysis.\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "nQKRfkMB8TSH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}